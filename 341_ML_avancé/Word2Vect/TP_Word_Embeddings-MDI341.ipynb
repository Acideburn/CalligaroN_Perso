{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrôle de version\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "# Packages nécessaires\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "# A la première utilisation de nltk, télécharger les données nécessaires\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (<span style=\"color:red\">TODO</span> pour signaler du contenu manquant)\n",
    "\n",
    "##  Word Embeddings : Représentations distribuées via l'hypothèse distributionelle\n",
    "\n",
    "**But**: On va chercher à obtenir des représentations denses (comme vecteurs de nombres réels) de mots (et éventuellement de phrases). Ces représentations ont vocation à être distribuées: ce sont des représentations non-locales. On représente un objet comme une combinaison de *features*, par opposition à l'attribution d'un symbole dédié: voir le travail fondateur d'entre autres, Geoffrey Hinton, sur le sujet: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n",
    "\n",
    "Le terme de représentation *distribuées* est très général, mais correspond à que l'on cherche à obtenir. L'enjeu est donc de pouvoir construire, automatiquement, de telles représentations.\n",
    "\n",
    "**Idée sous-jacente**: Elle est basée sur l'hypothèse distributionelle: les informations contextuelles suffisent à obtenir une représentation viable d'objets linguistiques.\n",
    " - *“For a large class of cases [...] the  meaning  of a word is  its  use in the  language.”* Wittgenstein (Philosophical Investigations, 43 - 1953)\n",
    " - *“You shall know a word by the company it keeps”*, Firth (\"A synopsis of linguistic theory 1930-1955.\" - 1957)\n",
    "\n",
    "Ainsi, on peut caractériser un mot par les mots qui l'accompagnent, via des comptes de co-occurences. Deux mots ayant un sens similaire auront une distribution contextuelle similaire et auront donc plus de chance d'apparaître dans des contextes similaires. Cette hypothèse peut servir de justification à l'application de statistiques à la sémantique (extraction d'information, analyse sémantique). Elle permet aussi une certaine forme de généralisation: on peut supposer que les informations que l'on a à propos d'un mot se généraliseront aux mots à la distribution similaire. \n",
    "\n",
    "**Motivation**: On cherche à obtenir des représentations distribuées pour pouvoir, de manière **efficace**:\n",
    "- Directement réaliser une analyse sémantique de surface.\n",
    "- S'en servir comme source d'informations pour d'autres modèles et applications liées au language, notamment pour l'analyse de sentiments. \n",
    "\n",
    "\n",
    "**Terminologie**: Attention à ne pas confondre l'idée de représentation *distribuée* et *distributionelle*. Le second indique en général (pour les mots) que la représentation a été obtenue strictement à partir de comptes de co-occurences, alors qu'on pourra utiliser des informations supplémentaires (labels de documents, tags de partie du discours, ...) pour construire des représentations distribuées. \n",
    "Les modèles qui permettent de construire ces représentations denses, sous forme de vecteurs, sont souvent appellés *vector spaces models*. On appelle aussi régulièrement ces représentations des *word embeddings*, car les mots sont embarqués (*embedded*) dans un espace vectoriel. En Français, on rencontre souvent le terme *plongements de mots* ou *plongements lexicaux*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenir une représentation: comptes d'occurences et de co-occurences\n",
    "\n",
    "Selon le type de corpus dont on dispose, on pourra obtenir différents types d'informations distributionelles. Si l'on a accès à une collection de documents, on pourra ainsi choisir de compter le nombre d'occurence de chaque mot dans chacun des documents, pour obtenir une matrice $mots \\times documents$: c'est sur ce principe qu'est construit **Tf-Idf** (vu au TP précédent). On va maintenant s'intéresser à un cas plus général: on dispose d'une grande quantité de données sous forme de texte, et on cherche à obtenir des représentations de mots sous forme de vecteurs de taille réduite, sans avoir besoin d'un découpage en documents ou catégories. \n",
    "\n",
    "Supposons qu'on dispose d'un corpus contenant $T$ mots différents. On va construire une matrice $\\mathbf{M}$ de taille $T \\times T$ qui contiendra le nombre de co-occurences entre les mots. Il y aura différents facteurs à considérer lors de la construction de cette matrice: \n",
    "- Comment définir le 'contexte' d'un mot, qui permettra de dire que les termes qu'il contient co-occurent avec ce mot ? On pourra choisir d'utiliser différentes échelles: le document, la phrase, le groupe nominal, ou tout simplement une fenêtre de $k$ mots, selon les informations que l'on cherche à capturer.\n",
    "*Encore une fois, si par exemple notre corpus est divisé en $D$ documents, on pourra même s'intéresser aux liens distributionnels entre mots et documents: chacun de ces $D$ documents agira comme un \"contexte\", et on construit une matrice d'occurences $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$.* \n",
    "- Comment quantifier l'importance des comptes ? Par exemple, on pourra donner un poids décroissant à une co-occurence selon la distance entre les deux mots concernés ($\\frac{1}{d+1}$ pour une séparation par $d$ mots).\n",
    "- Faut-il garder tous les mots qui apparaissent dans le corpus ? En général, non. On verra que pour les grands corpus, le nombre de mots différents $T$ est énorme. Deuxièmement, même si le nombre de mots est raisonnable, on ne possèdera que très peu d'information distributionelle sur les mots les plus rares, et la représentation obtenue sera à priori de mauvaise qualité. Il faudra se poser la question de comment filtrer ces mots, et de comment traiter les mots qu'on choisit de ne pas représenter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procédure\n",
    "\n",
    "Pour construire la matrice, on va dans un premier temps recueillir la liste des mots différents (ou le *vocabulaire* $V$) qui apparaissent dans le corpus sous forme de dictionaire {mots -> index}\n",
    "Puis, pour chaque terme $w$ du corpus,\n",
    "- On récupère l'index $i$ correspondant à l'aide de $V$\n",
    "- Pour chaque terme $w'$ du contexte de $w$, \n",
    "  + On récupère l'index $j$ correspondant à l'aide de $V$\n",
    "  + On incrémente $\\mathbf{M}_{i,j}$ par le poids correspondant, ou par $1$. \n",
    "  \n",
    "La procédure est très proche de celle qu'on a suivi au TP précédent, excepté qu'il faut maintenant compter les mots suivant leur apparition \n",
    "  \n",
    "#### Exemple\n",
    "\n",
    "On considère le corpus suivant: \n",
    "\n",
    "*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n",
    "\n",
    "On choisit de définir le contexte d'un mot comme la phrase à laquelle il appartient, et de ne pas utiliser de poids. \n",
    "On obtient la matrice suivante: \n",
    "\n",
    "|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
    "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
    "| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
    "| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
    "| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
    "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
    "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
    "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
    "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
    "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
    "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir un Vocabulaire:\n",
    "\n",
    "Cette fois, on va implémenter séparément une fonction retournant le vocabulaire. Il faudra ici pouvoir contrôler sa taille, que ce soit en indiquant un nombre maximum de mots, ou un nombre minimum d'occurences pour qu'on prenne en compte les mots. On ajoute, à la fin, un mot \"inconnu\" qui remplacera tous les mots qui n'apparaissent pas dans notre vocabulaire 'limité'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "walked\n",
      "down\n",
      "down\n",
      "the\n",
      "boulevard\n",
      "i\n",
      "walked\n",
      "down\n",
      "the\n",
      "avenue\n",
      "i\n",
      "ran\n",
      "down\n",
      "the\n",
      "boulevard\n",
      "i\n",
      "walk\n",
      "down\n",
      "the\n",
      "city\n",
      "i\n",
      "walk\n",
      "down\n",
      "the\n",
      "the\n",
      "avenue\n"
     ]
    }
   ],
   "source": [
    "for i in corpus :\n",
    "    for word in clean_and_tokenize(i) :\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 5,\n",
       " 'walked': 2,\n",
       " 'down': 6,\n",
       " 'the': 6,\n",
       " 'boulevard': 2,\n",
       " 'avenue': 2,\n",
       " 'ran': 1,\n",
       " 'walk': 2,\n",
       " 'city': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = {}\n",
    "##\n",
    "for sentence in corpus:\n",
    "    for word in clean_and_tokenize(sentence):\n",
    "        word_counts[word.lower()]=word_counts.get(word.lower(),0)+1\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(corpus, count_threshold=0, voc_threshold=10000):\n",
    "    \"\"\"    \n",
    "    Function using word counts to build a vocabulary\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
    "        voc_threshold (int): maximum size of the vocabulary - 0 (default) indicates there is no max\n",
    "    Returns:\n",
    "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
    "                                 values: indexes corresponding to each word sorted by frequency \n",
    "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
    "                                             values: word counts in the corpus\n",
    "        \"\"\"\n",
    "    word_counts = {}\n",
    "    ##\n",
    "    for sentence in corpus:\n",
    "        for word in clean_and_tokenize(sentence):\n",
    "            word_counts[word.lower()]=word_counts.get(word.lower(),0)+1\n",
    "    ##\n",
    "    filtered_word_counts={}\n",
    "    a=0\n",
    "    for key,val in word_counts.items():\n",
    "        if (val > count_threshold) & (val < voc_threshold):\n",
    "            filtered_word_counts[key]=val\n",
    "    word_counts,filtered_word_counts\n",
    "    ##\n",
    "    vocabulary = {}\n",
    "    vocabulary_word_counts = {}\n",
    "    ##\n",
    "    a = sorted(filtered_word_counts.items(), key=lambda t: t[1],reverse=True)\n",
    "    for i in range(len(filtered_word_counts)):\n",
    "        vocabulary[a[i][0]]=i\n",
    "        vocabulary_word_counts[a[i][0]]=a[i][1]\n",
    "    vocabulary['<UNK>']=i+1\n",
    "    vocabulary_word_counts['<UNK>']=0\n",
    "\n",
    "\n",
    "    return vocabulary, vocabulary_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down': 0, 'the': 1, 'i': 2, '<UNK>': 3}\n",
      "{'down': 6, 'the': 6, 'i': 5, '<UNK>': 0}\n",
      "{'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, '<UNK>': 9}\n",
      "{'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, '<UNK>': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example for testing:\n",
    "\n",
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "\n",
    "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
    "\n",
    "voc, counts = vocabulary(corpus)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir les co-occurences:\n",
    "\n",
    "La fonction prend en entrée le corpus (une liste de strings, correspondant aux documents, ou phrases) et un vocabulaire, ainsi que la taille de la fenêtre de contexte. On pourra aussi implémenter la solution la plus simple - que le contexte d'un mot soit le reste du document duquel il provient. \n",
    "Enfin, on pourra implémenter la possibilité de faire décroitre linéairement l'importance d'un mot du contexte à mesure qu'on s'éloigne du mot d'origine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurence_matrix(corpus, voc, window=0, distance_weighting=False):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        vocabulary (dictionary): words to use in the matrix\n",
    "        window (int): size of the context window; when 0, the context is the whole sentence\n",
    "        distance_weighting (bool): indicates if we use a weight depending on the distance between words for co-oc counts\n",
    "    Returns:\n",
    "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
    "    \"\"\" \n",
    "    l = len(voc)\n",
    "    M = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Obtenir la phrase:\n",
    "        sent = clean_and_tokenize(sent)\n",
    "        # Obtenir les indexs de la phrase grace au vocabulaire: \n",
    "        \"\"\" A compléter\"\"\"\n",
    "        sent_idx = [voc.get(word,len(voc)-1)for word in sent]\n",
    "        \"\"\" A compléter\"\"\"\n",
    "        # Parcourir les indexs de la phrase et ajouter 1 / dist(i,j) à M[i,j] si les mots d'index i et j apparaissent dans la même fenêtre. \n",
    "        for i, idx_i in enumerate(sent_idx):\n",
    "            # On vérifie que le mot est reconnu par le vocabulaire:\n",
    "            if idx_i > -1:\n",
    "                # Si on considère un contexte limité:\n",
    "                if window > 0:\n",
    "                    # On crée une liste qui contient les indexs de la fenêtre à gauche de l'index courant 'idx_i'\n",
    "                    \"\"\" A compléter\"\"\"\n",
    "                    l_ctx_idx = sent_idx[max(0,i-window):i]\n",
    "                    \"\"\" A compléter\"\"\"\n",
    "                # Si on considère que le contexte est la phrase entière:\n",
    "                else:\n",
    "                    # La liste qui contient le contexte à gauche du mot est plus facile à créer:\n",
    "                    \"\"\" A compléter\"\"\"\n",
    "                    l_ctx_idx = sent_idx[0:i]\n",
    "                    \"\"\" A compléter\"\"\"\n",
    "                # On parcourt cette liste et on update M[i,j]:    \n",
    "                for j, idx_j in enumerate(l_ctx_idx):\n",
    "                    # ... en s'assurant que le mot correspondant à 'idx_j' est reconnu par le vocabulaire\n",
    "                    if idx_j > -1:\n",
    "                        # Calcul du poids:\n",
    "                        if distance_weighting:\n",
    "                            \"\"\" A compléter\"\"\" \n",
    "                            weight = 1.0/(len(l_ctx_idx)-j)\n",
    "                            \"\"\" A compléter\"\"\" \n",
    "                        else:\n",
    "                            weight = 1.0\n",
    "                        M[idx_i, idx_j] += weight * 1.0\n",
    "                        M[idx_j, idx_i] += weight * 1.0\n",
    "    return M  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 7. 6. 3. 3. 2. 2. 1. 1. 0.]\n",
      " [7. 2. 6. 2. 2. 3. 3. 1. 1. 0.]\n",
      " [6. 6. 0. 2. 2. 2. 2. 1. 1. 0.]\n",
      " [3. 2. 2. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [3. 2. 2. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [2. 3. 2. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [2. 3. 2. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(co_occurence_matrix(corpus, voc, 0, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application à un vrai jeu de données\n",
    "\n",
    "On va chercher à obtenir ces comptes pour les données **imdb**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 documents\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "filenames_neg = sorted(glob(os.path.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(os.path.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
    "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))\n",
    "\n",
    "texts_red = texts[0::50]\n",
    "y_red = y [0::50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etude rapide des données\n",
    "\n",
    "On voudrait ici, avant de procéder, avoir une idée de ce que contiennent ces critiques de films. On va donc obtenir le vocabulaire (entier) et représenter les fréquences des mots, dans l'ordre (attention, il faudra utiliser une échelle logarithmique): on devrait retrouver la loi de Zipf. Cela nous permettra d'avoir une idée de la taille du vocabulaire qu'on pourra choisir : il s'agit de réaliser un compromis entre les ressources nécessaires (taille des objets en mémoire) et quantité d'informations qu'on peut en tirer (les mots rares peuvent apporter beaucoup d'informations, mais il est difficile d'en apprendre de bonnes représentations,car ils sont rares !)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would like to display the curve of word frequencies given their rank (index) in the vocabulary\n",
    "\n",
    "vocab, word_counts = vocabulary(texts)\n",
    "\n",
    "freq_word=[]\n",
    "for _,val in word_counts.items():\n",
    "    freq_word.append(val)\n",
    "\n",
    "indice_word= [i for i in range(len(vocab))]\n",
    "\n",
    "#\n",
    "# We can for example use the function plt.scatter()\n",
    "#en x l'indice de chaque mot, en y la fréquence de chaque mot\n",
    "#On trira l'ensemble pour avoir les mots les plus fréquent au début\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Word counts versus rank')\n",
    "#\n",
    "plt.scatter(x=indice_word,y=freq_word)\n",
    "#\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-1, 1e5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 93261\n",
      "Part of the corpus by taking the \"100\" most frequent words ?\n",
      "20.2%\n"
     ]
    }
   ],
   "source": [
    "# We would like to know how much of the data is represented by the 'k' most frequent words\n",
    "print('Vocabulary size: %i' % len(vocab))\n",
    "x=100\n",
    "print(f'Part of the corpus by taking the \"{x}\" most frequent words ?')\n",
    "#\n",
    "print(f\"{np.sum(freq_word[:x])/np.sum(freq_word)*100:.4}%\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat de l'analyse: on peut se contenter d'un vocabulaire de 10000, voire 5000 mots - c'est important, car cela va déterminer la taille des objets que l'on va manipuler. On va maintenant recréer la matrice de co-occurence avec différents paramètres. Cela peut-être long: si cela pose problème, travaillez avec un vocabulaire plus réduit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'through': 0,\n",
       " 'should': 1,\n",
       " 'back': 2,\n",
       " 'im': 3,\n",
       " 'real': 4,\n",
       " 'those': 5,\n",
       " 'watching': 6,\n",
       " 'years': 7,\n",
       " 'doesnt': 8,\n",
       " 'now': 9,\n",
       " 'actors': 10,\n",
       " 'though': 11,\n",
       " 'old': 12,\n",
       " 'thing': 13,\n",
       " 's': 14,\n",
       " 'didnt': 15,\n",
       " 'work': 16,\n",
       " 'new': 17,\n",
       " 'another': 18,\n",
       " 'before': 19,\n",
       " 'nothing': 20,\n",
       " 'funny': 21,\n",
       " 'actually': 22,\n",
       " 'makes': 23,\n",
       " 'director': 24,\n",
       " 'look': 25,\n",
       " 'find': 26,\n",
       " 'going': 27,\n",
       " 'few': 28,\n",
       " 'same': 29,\n",
       " 'part': 30,\n",
       " 'us': 31,\n",
       " 'every': 32,\n",
       " 'lot': 33,\n",
       " 'again': 34,\n",
       " 'cast': 35,\n",
       " 'quite': 36,\n",
       " 'cant': 37,\n",
       " 'thats': 38,\n",
       " 'down': 39,\n",
       " 'want': 40,\n",
       " 'things': 41,\n",
       " 'world': 42,\n",
       " 'pretty': 43,\n",
       " 'young': 44,\n",
       " 'seems': 45,\n",
       " 'around': 46,\n",
       " 'got': 47,\n",
       " 'horror': 48,\n",
       " 'fact': 49,\n",
       " '&': 50,\n",
       " 'take': 51,\n",
       " 'however': 52,\n",
       " 'big': 53,\n",
       " 'enough': 54,\n",
       " 'thought': 55,\n",
       " 'long': 56,\n",
       " 'series': 57,\n",
       " 'between': 58,\n",
       " 'both': 59,\n",
       " 'give': 60,\n",
       " 'may': 61,\n",
       " 'original': 62,\n",
       " 'own': 63,\n",
       " 'ive': 64,\n",
       " 'action': 65,\n",
       " 'right': 66,\n",
       " 'without': 67,\n",
       " 'always': 68,\n",
       " 'gets': 69,\n",
       " 'times': 70,\n",
       " 'comedy': 71,\n",
       " 'point': 72,\n",
       " 'must': 73,\n",
       " 'come': 74,\n",
       " 'isnt': 75,\n",
       " 'role': 76,\n",
       " 'saw': 77,\n",
       " 'almost': 78,\n",
       " 'interesting': 79,\n",
       " 'least': 80,\n",
       " 'family': 81,\n",
       " 'done': 82,\n",
       " 'whole': 83,\n",
       " 'theres': 84,\n",
       " 'bit': 85,\n",
       " 'music': 86,\n",
       " 'script': 87,\n",
       " 'far': 88,\n",
       " 'making': 89,\n",
       " 'guy': 90,\n",
       " 'feel': 91,\n",
       " 'minutes': 92,\n",
       " 'anything': 93,\n",
       " 'last': 94,\n",
       " 'might': 95,\n",
       " 'hes': 96,\n",
       " 'since': 97,\n",
       " 'performance': 98,\n",
       " 'tv': 99,\n",
       " 'probably': 100,\n",
       " 'am': 101,\n",
       " 'kind': 102,\n",
       " 'away': 103,\n",
       " 'rather': 104,\n",
       " 'yet': 105,\n",
       " 'worst': 106,\n",
       " 'girl': 107,\n",
       " 'day': 108,\n",
       " 'fun': 109,\n",
       " 'sure': 110,\n",
       " 'hard': 111,\n",
       " 'woman': 112,\n",
       " 'each': 113,\n",
       " 'found': 114,\n",
       " 'anyone': 115,\n",
       " 'played': 116,\n",
       " 'having': 117,\n",
       " 'our': 118,\n",
       " 'although': 119,\n",
       " 'believe': 120,\n",
       " 'course': 121,\n",
       " 'comes': 122,\n",
       " 'looking': 123,\n",
       " 'trying': 124,\n",
       " 'especially': 125,\n",
       " 'screen': 126,\n",
       " 'set': 127,\n",
       " 'goes': 128,\n",
       " 'shows': 129,\n",
       " 'looks': 130,\n",
       " 'place': 131,\n",
       " 'put': 132,\n",
       " 'book': 133,\n",
       " 'different': 134,\n",
       " 'ending': 135,\n",
       " 'money': 136,\n",
       " 'wasnt': 137,\n",
       " 'true': 138,\n",
       " 'sense': 139,\n",
       " 'once': 140,\n",
       " 'reason': 141,\n",
       " 'everything': 142,\n",
       " 'maybe': 143,\n",
       " 'actor': 144,\n",
       " 'dvd': 145,\n",
       " 'three': 146,\n",
       " 'worth': 147,\n",
       " 'main': 148,\n",
       " 'job': 149,\n",
       " 'year': 150,\n",
       " 'watched': 151,\n",
       " 'someone': 152,\n",
       " 'together': 153,\n",
       " 'plays': 154,\n",
       " 'play': 155,\n",
       " 'american': 156,\n",
       " 'takes': 157,\n",
       " 'said': 158,\n",
       " 'later': 159,\n",
       " 'effects': 160,\n",
       " 'seem': 161,\n",
       " 'instead': 162,\n",
       " 'beautiful': 163,\n",
       " 'house': 164,\n",
       " 'high': 165,\n",
       " 'himself': 166,\n",
       " 'night': 167,\n",
       " 'version': 168,\n",
       " 'john': 169,\n",
       " 'audience': 170,\n",
       " 'during': 171,\n",
       " 'everyone': 172,\n",
       " 'left': 173,\n",
       " 'special': 174,\n",
       " 'seeing': 175,\n",
       " 'half': 176,\n",
       " 'excellent': 177,\n",
       " 'star': 178,\n",
       " 'wife': 179,\n",
       " 'shot': 180,\n",
       " 'idea': 181,\n",
       " 'war': 182,\n",
       " 'less': 183,\n",
       " 'black': 184,\n",
       " 'nice': 185,\n",
       " 'mind': 186,\n",
       " 'simply': 187,\n",
       " 'read': 188,\n",
       " 'second': 189,\n",
       " 'father': 190,\n",
       " 'youre': 191,\n",
       " 'else': 192,\n",
       " 'kids': 193,\n",
       " 'help': 194,\n",
       " 'fan': 195,\n",
       " 'death': 196,\n",
       " 'poor': 197,\n",
       " 'completely': 198,\n",
       " 'used': 199,\n",
       " 'home': 200,\n",
       " 'line': 201,\n",
       " 'short': 202,\n",
       " 'dead': 203,\n",
       " 'men': 204,\n",
       " 'friends': 205,\n",
       " 'given': 206,\n",
       " 'either': 207,\n",
       " 'top': 208,\n",
       " 'try': 209,\n",
       " 'budget': 210,\n",
       " 'classic': 211,\n",
       " 'performances': 212,\n",
       " 'use': 213,\n",
       " 'wrong': 214,\n",
       " 'enjoy': 215,\n",
       " 'need': 216,\n",
       " 'rest': 217,\n",
       " 'hollywood': 218,\n",
       " 'low': 219,\n",
       " 'boring': 220,\n",
       " 'production': 221,\n",
       " 'full': 222,\n",
       " 'camera': 223,\n",
       " 'until': 224,\n",
       " 'along': 225,\n",
       " 'truly': 226,\n",
       " 'women': 227,\n",
       " 'video': 228,\n",
       " 'tell': 229,\n",
       " 'awful': 230,\n",
       " 'next': 231,\n",
       " 'couple': 232,\n",
       " 'stars': 233,\n",
       " 'start': 234,\n",
       " 'remember': 235,\n",
       " 'stupid': 236,\n",
       " 'came': 237,\n",
       " 'sex': 238,\n",
       " 'recommend': 239,\n",
       " 'mean': 240,\n",
       " 'perhaps': 241,\n",
       " 'others': 242,\n",
       " 'let': 243,\n",
       " 'moments': 244,\n",
       " 'wonderful': 245,\n",
       " 'understand': 246,\n",
       " 'small': 247,\n",
       " 'episode': 248,\n",
       " 'face': 249,\n",
       " 'playing': 250,\n",
       " 'school': 251,\n",
       " 'getting': 252,\n",
       " 'terrible': 253,\n",
       " 'written': 254,\n",
       " 'keep': 255,\n",
       " 'often': 256,\n",
       " 'doing': 257,\n",
       " 'style': 258,\n",
       " 'early': 259,\n",
       " 'perfect': 260,\n",
       " 'name': 261,\n",
       " 'human': 262,\n",
       " 'definitely': 263,\n",
       " 'gives': 264,\n",
       " 'itself': 265,\n",
       " 'lines': 266,\n",
       " 'person': 267,\n",
       " 'live': 268,\n",
       " 'become': 269,\n",
       " 'lost': 270,\n",
       " 'dialogue': 271,\n",
       " 'piece': 272,\n",
       " 'felt': 273,\n",
       " 'head': 274,\n",
       " 'finally': 275,\n",
       " 'case': 276,\n",
       " 'supposed': 277,\n",
       " 'liked': 278,\n",
       " 'couldnt': 279,\n",
       " 'yes': 280,\n",
       " 'title': 281,\n",
       " 'absolutely': 282,\n",
       " 'white': 283,\n",
       " 'boy': 284,\n",
       " 'against': 285,\n",
       " 'sort': 286,\n",
       " 'picture': 287,\n",
       " 'went': 288,\n",
       " 'entire': 289,\n",
       " 'worse': 290,\n",
       " 'waste': 291,\n",
       " 'cinema': 292,\n",
       " 'certainly': 293,\n",
       " 'shes': 294,\n",
       " 'problem': 295,\n",
       " 'evil': 296,\n",
       " 'hope': 297,\n",
       " 'entertaining': 298,\n",
       " 'called': 299,\n",
       " 'based': 300,\n",
       " 'loved': 301,\n",
       " 'several': 302,\n",
       " 'overall': 303,\n",
       " 'fans': 304,\n",
       " 'mother': 305,\n",
       " 'oh': 306,\n",
       " 'drama': 307,\n",
       " 'beginning': 308,\n",
       " 'mr': 309,\n",
       " 'id': 310,\n",
       " 'killer': 311,\n",
       " 'care': 312,\n",
       " 'becomes': 313,\n",
       " 'lives': 314,\n",
       " 'already': 315,\n",
       " 'dark': 316,\n",
       " 'direction': 317,\n",
       " 'under': 318,\n",
       " 'laugh': 319,\n",
       " 'guys': 320,\n",
       " 'seemed': 321,\n",
       " 'example': 322,\n",
       " 'friend': 323,\n",
       " 'turn': 324,\n",
       " 'throughout': 325,\n",
       " 'despite': 326,\n",
       " 'wanted': 327,\n",
       " '\\x96': 328,\n",
       " 'unfortunately': 329,\n",
       " 'children': 330,\n",
       " 'final': 331,\n",
       " 'fine': 332,\n",
       " 'girls': 333,\n",
       " 'sound': 334,\n",
       " 'heart': 335,\n",
       " 'history': 336,\n",
       " 'totally': 337,\n",
       " 'amazing': 338,\n",
       " 'guess': 339,\n",
       " 'humor': 340,\n",
       " 'wont': 341,\n",
       " 'lead': 342,\n",
       " 'writing': 343,\n",
       " 'wants': 344,\n",
       " 'quality': 345,\n",
       " 'close': 346,\n",
       " 'days': 347,\n",
       " 'youll': 348,\n",
       " 'son': 349,\n",
       " 'michael': 350,\n",
       " 'behind': 351,\n",
       " 'works': 352,\n",
       " 'tries': 353,\n",
       " 'side': 354,\n",
       " 'art': 355,\n",
       " 'game': 356,\n",
       " 'past': 357,\n",
       " 'able': 358,\n",
       " 'child': 359,\n",
       " 'ill': 360,\n",
       " 'turns': 361,\n",
       " 'hand': 362,\n",
       " 'act': 363,\n",
       " 'enjoyed': 364,\n",
       " 'theyre': 365,\n",
       " 'town': 366,\n",
       " 'flick': 367,\n",
       " 'favorite': 368,\n",
       " 'genre': 369,\n",
       " 'kill': 370,\n",
       " 'starts': 371,\n",
       " 'gave': 372,\n",
       " 'car': 373,\n",
       " 'run': 374,\n",
       " 'soon': 375,\n",
       " 'actress': 376,\n",
       " 'late': 377,\n",
       " 'eyes': 378,\n",
       " 'directed': 379,\n",
       " 'sometimes': 380,\n",
       " 'viewer': 381,\n",
       " 'ones': 382,\n",
       " 'horrible': 383,\n",
       " 'parts': 384,\n",
       " 'hour': 385,\n",
       " 'self': 386,\n",
       " 'brilliant': 387,\n",
       " 'themselves': 388,\n",
       " 'expect': 389,\n",
       " 'stories': 390,\n",
       " 'thinking': 391,\n",
       " 'blood': 392,\n",
       " 'stuff': 393,\n",
       " 'city': 394,\n",
       " 'obviously': 395,\n",
       " 'voice': 396,\n",
       " 'decent': 397,\n",
       " 'feeling': 398,\n",
       " 'highly': 399,\n",
       " 'etc': 400,\n",
       " 'fight': 401,\n",
       " 'myself': 402,\n",
       " 'matter': 403,\n",
       " 'slow': 404,\n",
       " 'type': 405,\n",
       " 'kid': 406,\n",
       " 'hell': 407,\n",
       " 'heard': 408,\n",
       " 'says': 409,\n",
       " 'age': 410,\n",
       " 'killed': 411,\n",
       " 'except': 412,\n",
       " 'roles': 413,\n",
       " 'god': 414,\n",
       " 'took': 415,\n",
       " 'moment': 416,\n",
       " 'leave': 417,\n",
       " 'cannot': 418,\n",
       " 'strong': 419,\n",
       " 'police': 420,\n",
       " 'writer': 421,\n",
       " 'violence': 422,\n",
       " 'hit': 423,\n",
       " 'stop': 424,\n",
       " 'anyway': 425,\n",
       " 'known': 426,\n",
       " 'happens': 427,\n",
       " 'particularly': 428,\n",
       " 'involved': 429,\n",
       " 'happened': 430,\n",
       " 'wouldnt': 431,\n",
       " 'extremely': 432,\n",
       " 'obvious': 433,\n",
       " 'chance': 434,\n",
       " 'living': 435,\n",
       " 'told': 436,\n",
       " 'daughter': 437,\n",
       " 'lack': 438,\n",
       " 'coming': 439,\n",
       " 'experience': 440,\n",
       " 'murder': 441,\n",
       " 'alone': 442,\n",
       " 'attempt': 443,\n",
       " 'b': 444,\n",
       " 'james': 445,\n",
       " 'including': 446,\n",
       " 'happen': 447,\n",
       " 'wonder': 448,\n",
       " 'interest': 449,\n",
       " 'complete': 450,\n",
       " 'cut': 451,\n",
       " 'gore': 452,\n",
       " 'brother': 453,\n",
       " 'ago': 454,\n",
       " 'please': 455,\n",
       " 'none': 456,\n",
       " 'group': 457,\n",
       " 'crap': 458,\n",
       " 'save': 459,\n",
       " 'simple': 460,\n",
       " 'score': 461,\n",
       " 'ok': 462,\n",
       " 'looked': 463,\n",
       " 'career': 464,\n",
       " 'number': 465,\n",
       " 'shown': 466,\n",
       " 'exactly': 467,\n",
       " 'song': 468,\n",
       " 'lets': 469,\n",
       " 'running': 470,\n",
       " 'annoying': 471,\n",
       " 'sad': 472,\n",
       " 'hero': 473,\n",
       " 'serious': 474,\n",
       " 'musical': 475,\n",
       " 'yourself': 476,\n",
       " 'hours': 477,\n",
       " 'taken': 478,\n",
       " 'released': 479,\n",
       " 'whose': 480,\n",
       " 'scary': 481,\n",
       " 'reality': 482,\n",
       " 'english': 483,\n",
       " 'seriously': 484,\n",
       " 'cinematography': 485,\n",
       " 'ends': 486,\n",
       " 'usually': 487,\n",
       " 'opening': 488,\n",
       " 'across': 489,\n",
       " 'jokes': 490,\n",
       " 'light': 491,\n",
       " 'possible': 492,\n",
       " 'david': 493,\n",
       " 'cool': 494,\n",
       " 'somewhat': 495,\n",
       " 'usual': 496,\n",
       " 'body': 497,\n",
       " 'today': 498,\n",
       " 'hilarious': 499,\n",
       " 'relationship': 500,\n",
       " 'started': 501,\n",
       " 'happy': 502,\n",
       " 'ridiculous': 503,\n",
       " 'level': 504,\n",
       " 'middle': 505,\n",
       " 'view': 506,\n",
       " 'change': 507,\n",
       " 'talking': 508,\n",
       " 'wish': 509,\n",
       " 'finds': 510,\n",
       " 'taking': 511,\n",
       " 'shots': 512,\n",
       " 'order': 513,\n",
       " 'documentary': 514,\n",
       " 'huge': 515,\n",
       " 'episodes': 516,\n",
       " 'power': 517,\n",
       " 'female': 518,\n",
       " 'saying': 519,\n",
       " 'opinion': 520,\n",
       " 'husband': 521,\n",
       " 'room': 522,\n",
       " 'mostly': 523,\n",
       " 'novel': 524,\n",
       " 'directors': 525,\n",
       " 'five': 526,\n",
       " 'important': 527,\n",
       " 'robert': 528,\n",
       " 'talent': 529,\n",
       " 'modern': 530,\n",
       " 'turned': 531,\n",
       " 'major': 532,\n",
       " 'call': 533,\n",
       " 'strange': 534,\n",
       " 'single': 535,\n",
       " 'word': 536,\n",
       " 'apparently': 537,\n",
       " 'rating': 538,\n",
       " 'disappointed': 539,\n",
       " 'events': 540,\n",
       " 'four': 541,\n",
       " 'songs': 542,\n",
       " 'due': 543,\n",
       " 'basically': 544,\n",
       " 'knows': 545,\n",
       " 'earth': 546,\n",
       " 'comic': 547,\n",
       " 'supporting': 548,\n",
       " 'knew': 549,\n",
       " 'clearly': 550,\n",
       " 'attention': 551,\n",
       " 'british': 552,\n",
       " 'non': 553,\n",
       " 'king': 554,\n",
       " 'class': 555,\n",
       " 'television': 556,\n",
       " 'fast': 557,\n",
       " 'future': 558,\n",
       " 'country': 559,\n",
       " 'arent': 560,\n",
       " 'cheap': 561,\n",
       " 'easily': 562,\n",
       " 'problems': 563,\n",
       " 'thriller': 564,\n",
       " 'silly': 565,\n",
       " 'words': 566,\n",
       " 'tells': 567,\n",
       " 'local': 568,\n",
       " 'miss': 569,\n",
       " 'sequence': 570,\n",
       " 'jack': 571,\n",
       " 'bring': 572,\n",
       " 'beyond': 573,\n",
       " 'entertainment': 574,\n",
       " 'whats': 575,\n",
       " 'straight': 576,\n",
       " 'paul': 577,\n",
       " 'upon': 578,\n",
       " 'romantic': 579,\n",
       " 'moving': 580,\n",
       " 'sets': 581,\n",
       " 'similar': 582,\n",
       " 'whether': 583,\n",
       " 'viewers': 584,\n",
       " 'falls': 585,\n",
       " 'oscar': 586,\n",
       " 'rock': 587,\n",
       " 'mystery': 588,\n",
       " 'eye': 589,\n",
       " 'review': 590,\n",
       " 'predictable': 591,\n",
       " 'talk': 592,\n",
       " 'giving': 593,\n",
       " 'appears': 594,\n",
       " 'needs': 595,\n",
       " 'enjoyable': 596,\n",
       " 'george': 597,\n",
       " 'within': 598,\n",
       " 'richard': 599,\n",
       " 'message': 600,\n",
       " 'near': 601,\n",
       " 'theater': 602,\n",
       " 'animation': 603,\n",
       " 'ten': 604,\n",
       " 'above': 605,\n",
       " 'stand': 606,\n",
       " 'nearly': 607,\n",
       " 'lady': 608,\n",
       " 'sequel': 609,\n",
       " 'theme': 610,\n",
       " 'feels': 611,\n",
       " 'bunch': 612,\n",
       " 'red': 613,\n",
       " 'havent': 614,\n",
       " 'herself': 615,\n",
       " 'mention': 616,\n",
       " 'add': 617,\n",
       " 'dull': 618,\n",
       " 'points': 619,\n",
       " 'release': 620,\n",
       " 'surprised': 621,\n",
       " 'lots': 622,\n",
       " 'ways': 623,\n",
       " 'team': 624,\n",
       " 'named': 625,\n",
       " 'using': 626,\n",
       " 'easy': 627,\n",
       " 'storyline': 628,\n",
       " 'begins': 629,\n",
       " 'actual': 630,\n",
       " 'working': 631,\n",
       " 'hate': 632,\n",
       " 'effort': 633,\n",
       " 'feature': 634,\n",
       " 'fantastic': 635,\n",
       " 'york': 636,\n",
       " 'minute': 637,\n",
       " 'follow': 638,\n",
       " 'die': 639,\n",
       " 'french': 640,\n",
       " 'tale': 641,\n",
       " 'clear': 642,\n",
       " 'stay': 643,\n",
       " 'among': 644,\n",
       " 'comments': 645,\n",
       " 'elements': 646,\n",
       " 'typical': 647,\n",
       " 'sister': 648,\n",
       " 'showing': 649,\n",
       " 'editing': 650,\n",
       " 'tried': 651,\n",
       " 'fall': 652,\n",
       " 're': 653,\n",
       " 'parents': 654,\n",
       " 'dialog': 655,\n",
       " 'famous': 656,\n",
       " 'certain': 657,\n",
       " 'avoid': 658,\n",
       " 'season': 659,\n",
       " 'filmed': 660,\n",
       " 'form': 661,\n",
       " 'buy': 662,\n",
       " 'means': 663,\n",
       " 'check': 664,\n",
       " 'figure': 665,\n",
       " 'soundtrack': 666,\n",
       " 'general': 667,\n",
       " 'material': 668,\n",
       " 'period': 669,\n",
       " 'sorry': 670,\n",
       " 'weak': 671,\n",
       " 'leads': 672,\n",
       " 'doubt': 673,\n",
       " 'crime': 674,\n",
       " 'somehow': 675,\n",
       " 'kept': 676,\n",
       " 'realistic': 677,\n",
       " 'viewing': 678,\n",
       " 'peter': 679,\n",
       " 'space': 680,\n",
       " 'greatest': 681,\n",
       " 'tom': 682,\n",
       " 'gone': 683,\n",
       " 'th': 684,\n",
       " 'dance': 685,\n",
       " 'brought': 686,\n",
       " 'third': 687,\n",
       " 'lame': 688,\n",
       " 'hear': 689,\n",
       " 'imagine': 690,\n",
       " 'atmosphere': 691,\n",
       " 'particular': 692,\n",
       " 'suspense': 693,\n",
       " 'zombie': 694,\n",
       " 'de': 695,\n",
       " 'sequences': 696,\n",
       " 'whos': 697,\n",
       " 'writers': 698,\n",
       " 'learn': 699,\n",
       " 'move': 700,\n",
       " 'reviews': 701,\n",
       " 'forget': 702,\n",
       " 'whatever': 703,\n",
       " 'indeed': 704,\n",
       " 'eventually': 705,\n",
       " 'rent': 706,\n",
       " 'poorly': 707,\n",
       " 'japanese': 708,\n",
       " 'sexual': 709,\n",
       " 'average': 710,\n",
       " 'lee': 711,\n",
       " 'premise': 712,\n",
       " 'note': 713,\n",
       " 'stage': 714,\n",
       " 'deal': 715,\n",
       " 'sit': 716,\n",
       " 'believable': 717,\n",
       " 'possibly': 718,\n",
       " 'wait': 719,\n",
       " 'surprise': 720,\n",
       " 'decided': 721,\n",
       " 'youve': 722,\n",
       " 'expected': 723,\n",
       " 'nature': 724,\n",
       " 'became': 725,\n",
       " 'subject': 726,\n",
       " 'difficult': 727,\n",
       " 'truth': 728,\n",
       " 'okay': 729,\n",
       " 'killing': 730,\n",
       " 'street': 731,\n",
       " 'free': 732,\n",
       " 'romance': 733,\n",
       " 'hot': 734,\n",
       " 'screenplay': 735,\n",
       " 'leaves': 736,\n",
       " 'nor': 737,\n",
       " 'needed': 738,\n",
       " 'reading': 739,\n",
       " 'question': 740,\n",
       " 'meets': 741,\n",
       " 'boys': 742,\n",
       " 'baby': 743,\n",
       " 'begin': 744,\n",
       " 'credits': 745,\n",
       " 'write': 746,\n",
       " 'situation': 747,\n",
       " 'meet': 748,\n",
       " 'memorable': 749,\n",
       " 'dramatic': 750,\n",
       " 'superb': 751,\n",
       " 'shame': 752,\n",
       " 'dog': 753,\n",
       " 'dr': 754,\n",
       " 'otherwise': 755,\n",
       " 'male': 756,\n",
       " 'open': 757,\n",
       " 'disney': 758,\n",
       " 'joe': 759,\n",
       " 'earlier': 760,\n",
       " 'forced': 761,\n",
       " 'badly': 762,\n",
       " 'sci': 763,\n",
       " 'dream': 764,\n",
       " 'unless': 765,\n",
       " 'fi': 766,\n",
       " 'emotional': 767,\n",
       " 'crazy': 768,\n",
       " 'weird': 769,\n",
       " 'acted': 770,\n",
       " 'realize': 771,\n",
       " 'older': 772,\n",
       " 'deep': 773,\n",
       " 'footage': 774,\n",
       " 'forward': 775,\n",
       " 'beauty': 776,\n",
       " 'interested': 777,\n",
       " 'society': 778,\n",
       " 'laughs': 779,\n",
       " 'comment': 780,\n",
       " 'keeps': 781,\n",
       " 'whom': 782,\n",
       " 'plus': 783,\n",
       " 'america': 784,\n",
       " 'features': 785,\n",
       " 'fantasy': 786,\n",
       " 'directing': 787,\n",
       " 'ask': 788,\n",
       " 'air': 789,\n",
       " 'development': 790,\n",
       " 'towards': 791,\n",
       " 'sounds': 792,\n",
       " 'quickly': 793,\n",
       " 'mess': 794,\n",
       " 'box': 795,\n",
       " 'perfectly': 796,\n",
       " 'worked': 797,\n",
       " 'creepy': 798,\n",
       " 'mark': 799,\n",
       " 'total': 800,\n",
       " 'unique': 801,\n",
       " 'plenty': 802,\n",
       " 'cheesy': 803,\n",
       " 'hands': 804,\n",
       " 'previous': 805,\n",
       " 'brings': 806,\n",
       " 'personal': 807,\n",
       " 'result': 808,\n",
       " 'setting': 809,\n",
       " 'monster': 810,\n",
       " 'effect': 811,\n",
       " 'incredibly': 812,\n",
       " 'imdb': 813,\n",
       " 'rate': 814,\n",
       " 'fire': 815,\n",
       " 'leading': 816,\n",
       " 'appear': 817,\n",
       " 'admit': 818,\n",
       " 'apart': 819,\n",
       " 'joke': 820,\n",
       " 'casting': 821,\n",
       " 'brothers': 822,\n",
       " 'background': 823,\n",
       " 'return': 824,\n",
       " 'christmas': 825,\n",
       " 'powerful': 826,\n",
       " 'business': 827,\n",
       " 'present': 828,\n",
       " 'create': 829,\n",
       " 'meant': 830,\n",
       " 'la': 831,\n",
       " 'telling': 832,\n",
       " 'battle': 833,\n",
       " 'girlfriend': 834,\n",
       " 'hardly': 835,\n",
       " 'pay': 836,\n",
       " 'political': 837,\n",
       " 'break': 838,\n",
       " 'bill': 839,\n",
       " 'potential': 840,\n",
       " 'gay': 841,\n",
       " 'various': 842,\n",
       " 'twist': 843,\n",
       " 'fighting': 844,\n",
       " 'cop': 845,\n",
       " 'secret': 846,\n",
       " 'dumb': 847,\n",
       " 'masterpiece': 848,\n",
       " 'portrayed': 849,\n",
       " 'fails': 850,\n",
       " 'era': 851,\n",
       " 'inside': 852,\n",
       " 'co': 853,\n",
       " 'jane': 854,\n",
       " 'reasons': 855,\n",
       " 'front': 856,\n",
       " 'outside': 857,\n",
       " 'ideas': 858,\n",
       " 'missing': 859,\n",
       " 'match': 860,\n",
       " 'western': 861,\n",
       " 'deserves': 862,\n",
       " 'expecting': 863,\n",
       " 'filmmakers': 864,\n",
       " 'fairly': 865,\n",
       " 'nudity': 866,\n",
       " 'rich': 867,\n",
       " 'odd': 868,\n",
       " 'manages': 869,\n",
       " 'social': 870,\n",
       " 'attempts': 871,\n",
       " 'success': 872,\n",
       " '$': 873,\n",
       " 'ben': 874,\n",
       " 'married': 875,\n",
       " 'recently': 876,\n",
       " 'talented': 877,\n",
       " 'villain': 878,\n",
       " 'flat': 879,\n",
       " 'list': 880,\n",
       " 'unlike': 881,\n",
       " 'further': 882,\n",
       " 'remake': 883,\n",
       " 'copy': 884,\n",
       " 'wrote': 885,\n",
       " 'cute': 886,\n",
       " 'william': 887,\n",
       " 'agree': 888,\n",
       " 'plain': 889,\n",
       " 'doctor': 890,\n",
       " 'sadly': 891,\n",
       " 'following': 892,\n",
       " 'cold': 893,\n",
       " 'office': 894,\n",
       " 'missed': 895,\n",
       " 'sweet': 896,\n",
       " 'pure': 897,\n",
       " 'mentioned': 898,\n",
       " 'crew': 899,\n",
       " 'incredible': 900,\n",
       " 'gun': 901,\n",
       " 'large': 902,\n",
       " 'produced': 903,\n",
       " 'caught': 904,\n",
       " 'members': 905,\n",
       " 'wasted': 906,\n",
       " 'ended': 907,\n",
       " 'filled': 908,\n",
       " 'pace': 909,\n",
       " 'popular': 910,\n",
       " 'science': 911,\n",
       " 'decides': 912,\n",
       " 'revenge': 913,\n",
       " 'waiting': 914,\n",
       " 'party': 915,\n",
       " 'hold': 916,\n",
       " 'considering': 917,\n",
       " 'public': 918,\n",
       " 'sees': 919,\n",
       " 'cartoon': 920,\n",
       " 'slightly': 921,\n",
       " 'tension': 922,\n",
       " 'mary': 923,\n",
       " 'uses': 924,\n",
       " 'created': 925,\n",
       " 'spent': 926,\n",
       " 'familiar': 927,\n",
       " 'neither': 928,\n",
       " 'suddenly': 929,\n",
       " 'water': 930,\n",
       " 'compared': 931,\n",
       " 'convincing': 932,\n",
       " 'clever': 933,\n",
       " 'intelligent': 934,\n",
       " 'kills': 935,\n",
       " 'entirely': 936,\n",
       " 'escape': 937,\n",
       " 'cause': 938,\n",
       " 'spirit': 939,\n",
       " 'bored': 940,\n",
       " 'moves': 941,\n",
       " 'fear': 942,\n",
       " 'dancing': 943,\n",
       " 'audiences': 944,\n",
       " 'choice': 945,\n",
       " 'language': 946,\n",
       " 'scott': 947,\n",
       " 'italian': 948,\n",
       " 'credit': 949,\n",
       " 'laughing': 950,\n",
       " 'cover': 951,\n",
       " 'books': 952,\n",
       " 'visual': 953,\n",
       " 'state': 954,\n",
       " 'violent': 955,\n",
       " 'island': 956,\n",
       " 'ultimately': 957,\n",
       " 'trouble': 958,\n",
       " 'zombies': 959,\n",
       " 'value': 960,\n",
       " 'successful': 961,\n",
       " 'cat': 962,\n",
       " 'speak': 963,\n",
       " 'century': 964,\n",
       " 'concept': 965,\n",
       " 'positive': 966,\n",
       " 'german': 967,\n",
       " 'animated': 968,\n",
       " 'singing': 969,\n",
       " 'biggest': 970,\n",
       " 'basic': 971,\n",
       " 'runs': 972,\n",
       " 'consider': 973,\n",
       " 'depth': 974,\n",
       " 'effective': 975,\n",
       " 'recent': 976,\n",
       " 'band': 977,\n",
       " 'adult': 978,\n",
       " 'force': 979,\n",
       " 'former': 980,\n",
       " 'store': 981,\n",
       " 'died': 982,\n",
       " 'control': 983,\n",
       " 'walk': 984,\n",
       " 'amusing': 985,\n",
       " 'common': 986,\n",
       " 'exciting': 987,\n",
       " 'spend': 988,\n",
       " 'portrayal': 989,\n",
       " 'focus': 990,\n",
       " 'appreciate': 991,\n",
       " 'producers': 992,\n",
       " 'younger': 993,\n",
       " 'solid': 994,\n",
       " 'trash': 995,\n",
       " 'rated': 996,\n",
       " 'adventure': 997,\n",
       " 'hair': 998,\n",
       " 'pointless': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_5k, word_counts_5k = vocabulary(texts, 0, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93193"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(vocab_5k)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.zeros((l,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18957, 18957)\n"
     ]
    }
   ],
   "source": [
    "M5dist = co_occurence_matrix(corpus, vocab_5k, window=5, distance_weighting=True)\n",
    "print(M5dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4000e+01 6.9000e+01 2.0000e+01 ... 0.0000e+00 0.0000e+00 2.3673e+04]\n"
     ]
    }
   ],
   "source": [
    "M20 = co_occurence_matrix(texts, vocab_5k, window=20, distance_weighting=False)\n",
    "print(M20[429])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M5dist = co_occurence_matrix(corpus, vocab_5k, window=5, distance_weighting=True)\n",
    "M20 = co_occurence_matrix(corpus, vocab_5k, window=20, distance_weighting=False)\n",
    "print(M5dist.shape)\n",
    "print(M20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(vocab_5k['cinema'])\n",
    "print(M5dist[429])\n",
    "print(M20[429])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison de vecteurs\n",
    "\n",
    "On peut se servir de ces vecteurs de très grande taille pour une analyse sémantique très basique: par exemple, en cherchant les plus proches voisins d'un mot. Cependant, il faudra faire attention aux distances qu'on utilise, liées à certaines métriques (Euclidiennes, Cosine) ou éventuellement d'autres liées à l'appartenance aux ensembles (Matching, Jaccard). La normalisation des vecteurs peut aussi jouer un rôle. Dans tous les cas, il faut bien faire attention à ne pas sur-interprêter ce type de résultats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec un contexte large, sans prendre en compte la distance entre les mots:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['very', 'what', 'more', 'there', 'even', 'up', 'when', 'story', 'which']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'very', 'overall', 'decent', 'pretty', 'not', 'quite', 'nice', 'comedy']]\n",
      "\n",
      "Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['very', 'great', 'just', 'bad', 'also', 'really', 'what', 'she', 'time']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'fine', 'wonderful', 'strange', 'simple', 'strong', 'serious', 'nice', 'solid']]\n"
     ]
    }
   ],
   "source": [
    "def euclidean(u, v):\n",
    "    return np.linalg.norm(u-v)\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / np.sqrt(u.dot(u))\n",
    "\n",
    "def cosine(u, v):\n",
    "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def print_neighbors(distance, voc, co_oc, mot, k=10):\n",
    "    inv_voc = {id: w for w, id in voc.items()}\n",
    "    neigh = NearestNeighbors(n_neighbors=k, algorithm='brute', metric=distance)\n",
    "    neigh.fit(co_oc) \n",
    "    dist, ind = neigh.kneighbors([co_oc[voc[mot]]])\n",
    "    print(\"Plus proches voisins de %s selon la distance '%s': \" % (mot, distance.__name__))\n",
    "    print([[inv_voc[i] for i in s[1:]] for s in ind])\n",
    "    \n",
    "print(\"Avec un contexte large, sans prendre en compte la distance entre les mots:\")    \n",
    "print_neighbors(euclidean, vocab_5k, M20, 'good')\n",
    "print_neighbors(cosine, vocab_5k, M20, 'good')\n",
    "print(\"\")\n",
    "print(\"Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\")    \n",
    "print_neighbors(euclidean, vocab_5k, M5dist, 'good')\n",
    "print_neighbors(cosine, vocab_5k, M5dist, 'good') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthodes de pondération des matrices\n",
    "\n",
    "**Motivation**: On ne se base pour l'instant que sur la fréquence (ou au mieux, une pondération de la fréquence) pour construire ces représentations. Comme on peut s'en douter, la fréquence seule n'est pas suffisante pour capturer des informations sémantiques intéressantes. On peut l'illustrer avec le phénomène des mots très fréquents qui apparaissent dans de nombreux contextes très différents, ou de mots qui apparaissent très souvent ensemble sans avoir nécessairement de lien sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalisation**: Très simple; il s'agit d'annuler l'influence de la magnitude des comptes sur la représentation.\n",
    "\n",
    "$$\\mathbf{m_{normalized}} = \\left[ \n",
    "   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "   \\ldots\n",
    "   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "\\right]$$\n",
    " \n",
    "**Pointwise Mutual Information**: Il s'agit d'évaluer à quel point la co-occurence des deux termes est *inattendue*. En effet, cette mesure correspond au ratio de la probabilité jointe des deux mots et du produit de leur probabilités individuelles:\n",
    "$$\n",
    "\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n",
    "$$\n",
    "La probabilité jointe des deux mots correspond au nombre de fois ou on les observe ensemble, divisé par le nombre total de co-occurences du corpus: \n",
    "$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
    "La probabilité individuelle d'un mot correspond simplement à sa fréquence, que l'on peut calculer en comptant toutes les co-occurences ou ce mot apparaît:\n",
    "$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
    "Ainsi,\n",
    "$$ \n",
    "\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n",
    "$$\n",
    "On calcule ainsi le décalage entre l'observation que l'on a fait dans notre corpus et la fréquence d'apparition de ces termes si on les considère indépendant - c'est à dire qu'on suppose que leur co-occurence est une coïncidence.\n",
    "\n",
    "Le principal problème avec cette mesure est qu'elle n'est pas adaptée au cas où l'on observe aucune co-occurence. Puisque la PMI est censée renvoyer une quantité positive si l'on observe plus de co-occurences que prévu, et négative si l'on en observe moins, on ne peut pas choisir de remplacer $\\log(0)$ par $0$. Une solution couramment utilisée est d'utiliser la **Positive PMI**, qui fixe toutes les valeurs négatives à $0$.\n",
    " \n",
    " $$\\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n",
    " \\begin{cases}\n",
    " \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n",
    " 0 & \\textrm{otherwise}\n",
    " \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(co_oc, positive=True):\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    sum_tot = sum_vec.sum()\n",
    "    with np.errstate(divide='ignore'):\n",
    "        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                   \n",
    "    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
    "    if positive:\n",
    "        pmi[pmi < 0] = 0.0\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec la PPMI:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_neighbors=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-489-635192a1f91c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Avec la PPMI:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_5k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPPMI5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'good'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_5k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPPMI5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'good'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_5k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPPMI20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'good'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-487-a3b4c283798f>\u001b[0m in \u001b[0;36mprint_neighbors\u001b[1;34m(distance, voc, co_oc, mot, k)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0minv_voc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mneigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'brute'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mneigh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mco_oc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mco_oc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Plus proches voisins de %s selon la distance '%s': \"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_unsupervised.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mnearest\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \"\"\"\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_algorithm_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 664\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n\u001b[1;32m--> 106\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m    107\u001b[0m             )\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "PPMI5 = pmi(M5dist)\n",
    "PPMI20 = pmi(M20)\n",
    "\n",
    "print(\"Avec la PPMI:\")    \n",
    "print_neighbors(euclidean, vocab_5k, PPMI5, 'good')\n",
    "print_neighbors(cosine, vocab_5k, PPMI5, 'good')\n",
    "print_neighbors(euclidean, vocab_5k, PPMI20, 'good')\n",
    "print_neighbors(cosine, vocab_5k, PPMI20, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**: Comme on l'a déjà vu, il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF). \n",
    "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice $termes \\times documents$. Ainsi, pour une matrice $\\mathbf{X}$ de $n$ termes et $d$ documents: \n",
    "\n",
    " $$\\text{TF}(X, i, j) = \\frac{X_{i,j}}{\\sum_{i=1}^{t} X_{i,j}} $$\n",
    " \n",
    " $$\\text{IDF}(X, i) = \\log\\left(\\frac{d}{|\\{j : X_{i,j} > 0\\}|}\\right)$$\n",
    " \n",
    " $$\\text{TF-IDF}(X, i, j) = \\text{TF}(X, i, j) \\cdot \\text{IDF}(X, i)$$\n",
    "\n",
    "\n",
    "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. Ainsi, l'appliquer aux co-occurences des mots les plus fréquents n'est à priori pas optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(co_oc):\n",
    "    \"\"\"\n",
    "    Inverse document frequencies applied to our co_oc matrices\n",
    "    \"\"\"\n",
    "    # IDF\n",
    "    d = float(co_oc.shape[1])\n",
    "    in_doc = co_oc.astype(bool).sum(axis=1)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        idfs = np.log(d / in_doc)\n",
    "    idfs[np.isinf(idfs)] = 0.0  # log(0) = 0\n",
    "    # TF\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    tfs = co_oc / sum_vec\n",
    "    return (tfs.T * idfs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec TF-IDF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_neighbors=10 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-491-9e90f90a87ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Avec TF-IDF:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_5k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFIDF5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'good'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_5k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFIDF5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'good'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-487-a3b4c283798f>\u001b[0m in \u001b[0;36mprint_neighbors\u001b[1;34m(distance, voc, co_oc, mot, k)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0minv_voc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mneigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'brute'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mneigh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mco_oc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mco_oc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Plus proches voisins de %s selon la distance '%s': \"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_unsupervised.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mnearest\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \"\"\"\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_algorithm_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 664\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n\u001b[1;32m--> 106\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m    107\u001b[0m             )\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "TFIDF5 = tfidf(M5dist)\n",
    "\n",
    "print(\"Avec TF-IDF:\")    \n",
    "print_neighbors(euclidean, vocab_5k, TFIDF5, 'good')\n",
    "print_neighbors(cosine, vocab_5k, TFIDF5, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de co-occurences : Réduction de dimension\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Il s'agit non seulement de réduire la taille de données (ainsi, on traitera des vecteurs de dimension réduite, plutôt que de travailler avec des vecteurs de la taille du vocabulaire) mais aussi de mettre en évidence des relations de plus haut niveau entre les mots: en réduisant leurs représentations aux dimensions qui *les plus importantes* des données, on se retrouve à *généraliser* certaines propriétés entre les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réduction de dimension via SVD \n",
    "\n",
    "Une matrice est une transformation linéaire: y appliquer une SVD, c'est décomposer notre transformation linéaire en un produit de transformations linéaires de différents types. Il va s'agir d'effectuer un changement de base, et de replacer nos données dans un espace ou chacune des coordonnées sont inchangées par la transformation effectuée. Ainsi, on décompose la matrice $\\mathbf{M}$ en trois matrices:\n",
    "\n",
    "$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n",
    "\n",
    "Les matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, et $\\mathbf{V}$ ont les propriétés suivantes:\n",
    "- $\\mathbf{U}$ et $\\mathbf{V}$ sont des matrices orthogonales ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ et $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). Elles contiennent les vecteurs propres à gauche et à droite de $\\mathbf{M}$.\n",
    "- $\\mathbf{\\lambda}$ est une matrice diagonale: attention, elle n'est pas forcément carrée. Les coefficients de la diagonale sont les valeurs propres de $\\mathbf{M}$.\n",
    "\n",
    "Ainsi, les dmensions *les plus importantes* correspondent aux plus grandes valeurs propres. Réduire nos données à une dimension $k$ correspond à ne garder que les vecteurs correspondant aux $k$ premières valeurs propres - et cela revient à prendre les $k$ premiers vecteurs de la matrice $U$. \n",
    "On utilise ici ```TruncatedSVD``` du package ```scikit-learn```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "SVDEmbeddings = svd.fit_transform(M5dist)\n",
    "print(SVDEmbeddings.shape)\n",
    "SVDEmbeddings[vocab_5k['UNK']]\n",
    "\n",
    "print_neighbors(euclidean, vocab_5k, SVDEmbeddings, 'good')\n",
    "print_neighbors(cosine, vocab_5k, SVDEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Lorsque l'on applique cette méthode à la matrice des comptes $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$, on obtient la méthode appellée **Latent Semantic Analysis**, pour la détection de composantes latentes (sémantiques) permettant de regrouper les documents.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation en deux dimensions\n",
    "\n",
    "On va maintenant utiliser **l'analyse en composantes principales** (PCA) pour visualiser nos données en 2 dimensions.  Cela revient à appliquer la SVD à la matrice de covariance des données, pour que les directions principales soient indépendantes les unes des autres et maximisent la variance des données.\n",
    "On utilise la classe ```PCA``` du package ```scikit-learn```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "Emb = pca.fit_transform(M5dist)\n",
    "\n",
    "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
    "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
    "         'film', 'movie', 'oscar', 'award']\n",
    "ind_words = [vocab_5k[w] for w in words]\n",
    "x_words = [Emb[ind,0] for ind in ind_words]\n",
    "y_words = [Emb[ind,1] for ind in ind_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_words, y_words)\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Norm5 = M5dist / np.linalg.norm(M5dist, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "Emb = pca.fit_transform(Norm5)\n",
    "\n",
    "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
    "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
    "         'film', 'movie', 'oscar', 'award']\n",
    "ind_words = [vocab_5k[w] for w in words]\n",
    "x_words = [Emb[ind,0] for ind in ind_words]\n",
    "y_words = [Emb[ind,1] for ind in ind_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_words, y_words)\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenir une représentation: algorithmes couramment utilisés\n",
    "\n",
    "L'idée, ici, est de définir un ensemble de représentations ${w_{i}}_{i=1}^{V}$, de dimension prédéfinie $d$ (ici, on travaillera avec $d = 300$), pour tous les mots $i$ du vocabulaire $V$ - puis **d'entraîner** ces représentations pour qu'elles correspondent à ce que l'on souhaite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove\n",
    "\n",
    "L'objectif défini par Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) est d'apprendre des vecteurs $w_{i}$ et $w_{k}$ de façon à ce que leur produit scalaire correspondent au logarithme de leur **Pointwise Mutual Information**: \n",
    "\n",
    "\n",
    "$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n",
    "\n",
    "\n",
    "Dans l'article, l'obtention de cet objectif est minutieusement justifié par un raisonnement sur les opérations que l'on veut effectuer avec ces vecteurs et les propriétés qu'ils devraient avoir - notamment, une symétrie entre les lignes et les colonnes (voir l'article pour plus de détails).  \n",
    "L'objectif final obtenu est le suivant, où $M$ est la matrice de co-occurences:\n",
    "\n",
    "\n",
    "$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n",
    "  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n",
    "  \n",
    " \n",
    "Ici, $f$ est une fonction de *mise à l'échelle* qui permet de diminuer l'importance des comptes de co-occurences les plus fréquents: \n",
    "\n",
    "\n",
    "$$f(x) \n",
    "\\begin{cases}\n",
    "(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n",
    "1 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "En général, on choisit $\\alpha=0.75$ et $x_{\\max} = 100$, même si ces paramètres peuvent nécessiter un changement selon les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant utilise l'API de gensim pour récupérer des représentations pré-entrainées (Il est normal que le chargement soit long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut extraire la matrice des embeddings ainsi, et vérifier sa taille:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_glove_embeddings = loaded_glove_model.vectors\n",
    "print(loaded_glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit donc qu'il y a $400.000$ mots représentés, et que les embeddings sont de dimension $300$. On définit une fonction qui nous renvoie, à partir du modèle chargé, le vocabulaire et la matrice des embeddings suivant les structures que l'on a utilisé auparavant. On ajoute, ici encore, un mot inconnu ```'UNK'``` au cas où se trouve dans nos données des mots qui ne font pas parti des $400.000$ mots représentés ici. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_voc_and_embeddings(glove_model):\n",
    "    voc = {word : index for word, index in enumerate(glove_model.index2word)}\n",
    "    voc['UNK'] = len(voc)\n",
    "    embeddings = glove_model.vectors\n",
    "    return voc, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de comparer 'à jeu égal' les représentations chargées ici et celles que l'on a produite, il faudrait utiliser le même vocabulaire. Dans ce but, je réutilise le code qui suit pour créer un vocabulaire de $5000$ mots à partir des données exactement comme hier, et j'ajoute à la fin une fonction qui renvoie la matrices des représentations chargées avec Glove pour ces $5000$ mots seulement, dans le bon ordre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_adapted_embeddings(glove_model, input_voc):\n",
    "    keys = {i: glove_model.vocab.get(w, None) for w, i in input_voc.items()}\n",
    "    index_dict = {i: key.index for i, key in keys.items() if key is not None}\n",
    "    embeddings = np.zeros((len(input_voc),glove_model.vectors.shape[1]))\n",
    "    for i, ind in index_dict.items():\n",
    "        embeddings[i] = glove_model.vectors[ind]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction prend donc en entrée le modèle chargé à l'aide de l'API Gensim, ainsi qu'un vocabulaire que nous avons créé nous même, et renvoie la matrice d'embeddings tiré du modèle chargé, pour les mots notre vocabulaire et dans le bon ordre.\n",
    "Remarque: les mots inconnus sont représentés par le vecteur nul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GloveEmbeddings.shape)\n",
    "GloveEmbeddings[vocab_5k['UNK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good')\n",
    "print_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "\n",
    "#### Le modèle skip-gram\n",
    "\n",
    "Le modèle skip-gram de base estime les probabilités d'une paire de mots $(i, j)$ d'apparaître ensemble:\n",
    "\n",
    "\n",
    "$$P(j \\mid i) = \\frac{\\exp(w_{i} c_{j})}{\\sum_{j'\\in V}\\exp(w_{i} c_{j'})}$$\n",
    "\n",
    "\n",
    "où $w_{i}$ est le vecteur ligne (du mot) $i$ et $c_{j}$ est le vecteur colonne (d'un mot du contexte) $j$. L'objectif est de minimiser la quantité suivante: \n",
    "\n",
    "\n",
    "$$ -\\sum_{i=1}^{m} \\sum_{k=1}^{|V|} \\textbf{1}\\{o_{i}=k\\} \\log \\frac{\\exp(w_{i} c_{k})}{\\sum_{j=1}^{|V|} \\exp(w_{i} c_{j})}$$\n",
    "\n",
    "\n",
    "ou $V$ est le vocabulaire.\n",
    "Les entrées $w_{i}$ sont les représentations des mots, que l'on met à jour pendant l'entraînement, et la sortie est un vecteur *one-hot* $o$, qui ne contient qu'un seul $1$ et des $0$. Par exemple, si `good` est le  47ème mot du vocabulaire, la sortie $o$ pour un exemple ou `good` est le mot à prédire consistera en des $0$s partout sauf un $1$ en 47ème position du vecteur. `good` sera le mot à prédire lorsque l'entrée $w$ sera un mot de son contexte\n",
    "On obient donc cette sortie avec softmax standard - on ajoute un terme de biais $b$.\n",
    "\n",
    "\n",
    "$$ o = \\textbf{softmax}(w_{i}C + b)$$\n",
    "\n",
    "\n",
    "Si l'on utilise l'ensemble des représentations pour tout le vocabulaire (la matrice $W$) comme entrée, on obtient \n",
    "\n",
    "\n",
    "$$ O = \\textbf{softmax}(WC + b)$$\n",
    "\n",
    "\n",
    "et on revient ainsi à l'idée centrale de toutes nos méthodes: on cherche à obtenir des représentations de mots à partir de comptes de co-occurences. Ici, on entraîne les paramètres contenus dans $W$ et $C$, deux matrices représentants les mots en dimension réduite (300) de façon à ce que leur produit scalaire soit le plus proche possible des co-occurences observées dans les données, à l'aide d'un objectif de maximum de vraisemblance.\n",
    "\n",
    "#### Le skip gram avec negative sampling\n",
    "\n",
    "L'entraînement du modèle skip-gram implique de calculer une somme sur l'ensemble du vocabulaire, à cause du **softmax**. Dès que la taille du vocabulaire augmente, cela devient infaisalbe. Afin de rendre les calculs plus rapides, on change l'objectif et on utilise la méthode du *negative sampling* (ou, celle, très proche, du *noise contrastive estimation*).\n",
    "\n",
    "\n",
    "Si on note $\\mathcal{D}$ l'ensemble des données et que l'on not $\\mathcal{D}'$ un ensemble de paires de mots qui ne sont **pas** dans les données (et qu'en pratique, l'on tire aléatoirement, l'objectif est:\n",
    "\n",
    "\n",
    "$$\\sum_{i, j \\in \\mathcal{D}}-\\log\\sigma(w_{i}c_{j}) + \\sum_{i, j \\in \\mathcal{D}'}\\log\\sigma(w_{i}c_{j})$$\n",
    "\n",
    "\n",
    "ou $\\sigma$ est la fonction d'activation sigmoide $\\frac{1}{1 + \\exp(-x)}$.\n",
    "Une pratique commune est de générer les paires de $\\mathcal{D}'$ de manière proportionelle aux fréquences des mots dans les données d'entraînement (ce qu'on appelle la distribution unigramme):\n",
    "\n",
    "\n",
    "$$P(w) = \\frac{\\textbf{T}(w)^{0.75}}{\\sum_{w'\\in V} \\textbf{T}(w')}$$\n",
    "\n",
    "\n",
    "Bien que différente, cette nouvelle fonction objectif est une approximation suffisante de la précédente, et est basée sur le même principe. De nombreuses recherches ont été effectuées sur cet objectif: par exemple, [Levy and Golberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) démontre que l'objectif calcule la matrice de PMI décalé d'une valeur constante. On peut aussi voir [Cotterell et al. 2017](https://aclanthology.coli.uni-saarland.de/papers/E17-2028/e17-2028) pour une interprétation de l'algorithme comme une variante de la PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser la bibliothèque ```gensim``` pour son implémentation de word2vec en python. On va devoir en faire une utilisation un peu spécifique, puisqu'on veut conserver le même vocabulaire qu'auparavant: on va d'abord créer la classe, puis récupérer le vocabulaire qu'on a utilisé plus haut. \n",
    "Pour ne pas à avoir à mettre toutes les données en mémoire d'un coup, on définit un générateur, qui prendra toutes les données en entrée et les pré-traitera et renverra à la classe ```Word2Vec``` phrase par phrase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(size=300,\n",
    "                 window=5,\n",
    "                 null_word=1,\n",
    "                 iter=30)\n",
    "model.build_vocab_from_freq(word_counts_5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_generator(large_corpus):\n",
    "    for line in large_corpus:\n",
    "        yield clean_and_tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(preprocess_generator(corpus[:]), total_examples=10, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2VEmbeddings = model.wv.vectors\n",
    "print(W2VEmbeddings.shape)\n",
    "W2VEmbeddings[vocab_5k['UNK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, vocab_5k, W2VEmbeddings, 'good')\n",
    "print_neighbors(cosine, vocab_5k, W2VEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application à l'analyse de sentiments\n",
    "\n",
    "On va maintenant utiliser ces représentations pour l'analyse de sentiments. \n",
    "Le modèle de base, comme dans le TP précédent, sera construit en deux étapes:\n",
    "- Une fonction permettant d'obtenir des représentations vectorielles des critiques, à partir des textes, du vocabulaire, et des représentations vectorielles des mots. Une telle fonction (à compléter ci-dessous) va associer à chaque mot d'une critique son embeddings, et créer la représentation pour l'ensemble de la phrase en sommant ces embeddings.\n",
    "- Un classifieur qui prendra ces représentations en entrée et réalisera une prédiction. Pour le réaliser, on pourra utiliser d'abord la régression logistique ```LogisticRegression``` de ```scikit-learn```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n",
    "    \"\"\"\n",
    "    Represent the sentences as a combination of the vector of its words.\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : a list of sentences   \n",
    "    vocabulary : dict\n",
    "        From words to indexes of vector.\n",
    "    embeddings : Matrix containing word representations\n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise, \n",
    "        like `np.mean`, `np.sum`, or `np.prod`. \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
    "    \"\"\"\n",
    "    #\n",
    "    # A compléter !\n",
    "    # \n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Exemple avec les embeddings obtenus via Glove\n",
    "rep = sentence_representations(corpus, vocab_5k, GloveEmbeddings)\n",
    "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant comparer l'ensemble des méthodes, et notamment répondre aux questions suivantes:\n",
    "- Pourquoi peut-on s'attendre à ce que les résultats obtenus avec les embeddings extraits des représentations pré-apprises avec Gl0ve soient bien meilleurs que les autres ? Quel serait le moyen de comparer de manière 'juste' Gl0ve avec les autres méthodes d'apprentissage de représentations ?\n",
    "- Quelle matrice permet d'obtenir les meilleures représentations via SVD ? (Co-occurences, Tf-Idf, PPMI ..)\n",
    "- Word2Vec est difficile à paramétrer. Essayez d'améliorer les représentations en changeant la taille du contexte, le nombre d'itérations ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
