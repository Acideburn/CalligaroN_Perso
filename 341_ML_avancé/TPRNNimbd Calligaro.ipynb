{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBFqSEkKqpCN"
   },
   "source": [
    "# TP RNN \n",
    "# Using Many-to-One for movie rating predicton\n",
    "\n",
    "Last edit: 2019/01/15 - TP by geoffroy.peeters@telecom-paristech.fr\n",
    "\n",
    "### Objective:\n",
    "We will implement two different networks to perform automatic rating (0 or 1) of a movie given the text of its review.\n",
    "We will use the ```imdb``` (internet movie database) dataset.\n",
    "\n",
    "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmkCSNaXLqjh"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOqjzDwioJj9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5Yp4OQVvUtr"
   },
   "source": [
    "## Parameters of the model\n",
    "\n",
    "-  We only consider the ```top_words``` first words in the word dictionary\n",
    "- We truncate/zerp-pad each sequence a length ```max_review_length````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4C_Pv7rYvRkM"
   },
   "outputs": [],
   "source": [
    "top_words = 5000 \n",
    "max_review_length = 100\n",
    "INDEX_FROM = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsNcRimyLzgP"
   },
   "source": [
    "## Import IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5Gfe1ex8oN8Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# Import the IMDB data and only consider the ``top_words``` most used words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=INDEX_FROM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSc5LmksOLyr"
   },
   "source": [
    "## Data content\n",
    "\n",
    "- ```X_train``` and ```X_test``` are numpy arrays of lists. \n",
    "  - each item in a list is the index in the word dictionary. So that a list is the sequence of index of words.\n",
    "\n",
    "- ```y_train``` and ```y_test``` are a numpy arrays of the same dimension as ```X_train``` and ```X_test``` \n",
    "  - they contains the values 0 (bad movie) or 1 (good movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "WouODCPrtiuu",
    "outputId": "ba70767b-3429-448e-921c-a90eae1ef56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X_train): <class 'numpy.ndarray'>\n",
      "number of training sequences: X_train.shape: (25000,)\n",
      "type(X_train[0]): <class 'list'>\n",
      "length of the first training sequence: len(X_train[0]): 218\n",
      "length of the second training sequence: len(X_train[0]): 189\n",
      "list of data of the first training sequence: X_train[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "maximum length of a training sequence: 2494\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"type(X_train):\", type(X_train))\n",
    "print(\"number of training sequences: X_train.shape:\", X_train.shape)\n",
    "print(\"type(X_train[0]):\",type(X_train[0]))\n",
    "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
    "print(\"length of the second training sequence: len(X_train[0]):\",len(X_train[1]))\n",
    "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
    "len_list = [len(train) for train in X_train]\n",
    "print(\"maximum length of a training sequence:\", max(len_list))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(len_list, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of how the reviews are encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
     ]
    }
   ],
   "source": [
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34704: 'fawn',\n",
       " 52009: 'tsukino',\n",
       " 52010: 'nunnery',\n",
       " 16819: 'sonja',\n",
       " 63954: 'vani',\n",
       " 1411: 'woods',\n",
       " 16118: 'spiders',\n",
       " 2348: 'hanging',\n",
       " 2292: 'woody',\n",
       " 52011: 'trawling',\n",
       " 52012: \"hold's\",\n",
       " 11310: 'comically',\n",
       " 40833: 'localized',\n",
       " 30571: 'disobeying',\n",
       " 52013: \"'royale\",\n",
       " 40834: \"harpo's\",\n",
       " 52014: 'canet',\n",
       " 19316: 'aileen',\n",
       " 52015: 'acurately',\n",
       " 52016: \"diplomat's\",\n",
       " 25245: 'rickman',\n",
       " 6749: 'arranged',\n",
       " 52017: 'rumbustious',\n",
       " 52018: 'familiarness',\n",
       " 52019: \"spider'\",\n",
       " 68807: 'hahahah',\n",
       " 52020: \"wood'\",\n",
       " 40836: 'transvestism',\n",
       " 34705: \"hangin'\",\n",
       " 2341: 'bringing',\n",
       " 40837: 'seamier',\n",
       " 34706: 'wooded',\n",
       " 52021: 'bravora',\n",
       " 16820: 'grueling',\n",
       " 1639: 'wooden',\n",
       " 16821: 'wednesday',\n",
       " 52022: \"'prix\",\n",
       " 34707: 'altagracia',\n",
       " 52023: 'circuitry',\n",
       " 11588: 'crotch',\n",
       " 57769: 'busybody',\n",
       " 52024: \"tart'n'tangy\",\n",
       " 14132: 'burgade',\n",
       " 52026: 'thrace',\n",
       " 11041: \"tom's\",\n",
       " 52028: 'snuggles',\n",
       " 29117: 'francesco',\n",
       " 52030: 'complainers',\n",
       " 52128: 'templarios',\n",
       " 40838: '272',\n",
       " 52031: '273',\n",
       " 52133: 'zaniacs',\n",
       " 34709: '275',\n",
       " 27634: 'consenting',\n",
       " 40839: 'snuggled',\n",
       " 15495: 'inanimate',\n",
       " 52033: 'uality',\n",
       " 11929: 'bronte',\n",
       " 4013: 'errors',\n",
       " 3233: 'dialogs',\n",
       " 52034: \"yomada's\",\n",
       " 34710: \"madman's\",\n",
       " 30588: 'dialoge',\n",
       " 52036: 'usenet',\n",
       " 40840: 'videodrome',\n",
       " 26341: \"kid'\",\n",
       " 52037: 'pawed',\n",
       " 30572: \"'girlfriend'\",\n",
       " 52038: \"'pleasure\",\n",
       " 52039: \"'reloaded'\",\n",
       " 40842: \"kazakos'\",\n",
       " 52040: 'rocque',\n",
       " 52041: 'mailings',\n",
       " 11930: 'brainwashed',\n",
       " 16822: 'mcanally',\n",
       " 52042: \"tom''\",\n",
       " 25246: 'kurupt',\n",
       " 21908: 'affiliated',\n",
       " 52043: 'babaganoosh',\n",
       " 40843: \"noe's\",\n",
       " 40844: 'quart',\n",
       " 362: 'kids',\n",
       " 5037: 'uplifting',\n",
       " 7096: 'controversy',\n",
       " 21909: 'kida',\n",
       " 23382: 'kidd',\n",
       " 52044: \"error'\",\n",
       " 52045: 'neurologist',\n",
       " 18513: 'spotty',\n",
       " 30573: 'cobblers',\n",
       " 9881: 'projection',\n",
       " 40845: 'fastforwarding',\n",
       " 52046: 'sters',\n",
       " 52047: \"eggar's\",\n",
       " 52048: 'etherything',\n",
       " 40846: 'gateshead',\n",
       " 34711: 'airball',\n",
       " 25247: 'unsinkable',\n",
       " 7183: 'stern',\n",
       " 52049: \"cervi's\",\n",
       " 40847: 'dnd',\n",
       " 11589: 'dna',\n",
       " 20601: 'insecurity',\n",
       " 52050: \"'reboot'\",\n",
       " 11040: 'trelkovsky',\n",
       " 52051: 'jaekel',\n",
       " 52052: 'sidebars',\n",
       " 52053: \"sforza's\",\n",
       " 17636: 'distortions',\n",
       " 52054: 'mutinies',\n",
       " 30605: 'sermons',\n",
       " 40849: '7ft',\n",
       " 52055: 'boobage',\n",
       " 52056: \"o'bannon's\",\n",
       " 23383: 'populations',\n",
       " 52057: 'chulak',\n",
       " 27636: 'mesmerize',\n",
       " 52058: 'quinnell',\n",
       " 10310: 'yahoo',\n",
       " 52060: 'meteorologist',\n",
       " 42580: 'beswick',\n",
       " 15496: 'boorman',\n",
       " 40850: 'voicework',\n",
       " 52061: \"ster'\",\n",
       " 22925: 'blustering',\n",
       " 52062: 'hj',\n",
       " 27637: 'intake',\n",
       " 5624: 'morally',\n",
       " 40852: 'jumbling',\n",
       " 52063: 'bowersock',\n",
       " 52064: \"'porky's'\",\n",
       " 16824: 'gershon',\n",
       " 40853: 'ludicrosity',\n",
       " 52065: 'coprophilia',\n",
       " 40854: 'expressively',\n",
       " 19503: \"india's\",\n",
       " 34713: \"post's\",\n",
       " 52066: 'wana',\n",
       " 5286: 'wang',\n",
       " 30574: 'wand',\n",
       " 25248: 'wane',\n",
       " 52324: 'edgeways',\n",
       " 34714: 'titanium',\n",
       " 40855: 'pinta',\n",
       " 181: 'want',\n",
       " 30575: 'pinto',\n",
       " 52068: 'whoopdedoodles',\n",
       " 21911: 'tchaikovsky',\n",
       " 2106: 'travel',\n",
       " 52069: \"'victory'\",\n",
       " 11931: 'copious',\n",
       " 22436: 'gouge',\n",
       " 52070: \"chapters'\",\n",
       " 6705: 'barbra',\n",
       " 30576: 'uselessness',\n",
       " 52071: \"wan'\",\n",
       " 27638: 'assimilated',\n",
       " 16119: 'petiot',\n",
       " 52072: 'most\\x85and',\n",
       " 3933: 'dinosaurs',\n",
       " 355: 'wrong',\n",
       " 52073: 'seda',\n",
       " 52074: 'stollen',\n",
       " 34715: 'sentencing',\n",
       " 40856: 'ouroboros',\n",
       " 40857: 'assimilates',\n",
       " 40858: 'colorfully',\n",
       " 27639: 'glenne',\n",
       " 52075: 'dongen',\n",
       " 4763: 'subplots',\n",
       " 52076: 'kiloton',\n",
       " 23384: 'chandon',\n",
       " 34716: \"effect'\",\n",
       " 27640: 'snugly',\n",
       " 40859: 'kuei',\n",
       " 9095: 'welcomed',\n",
       " 30074: 'dishonor',\n",
       " 52078: 'concurrence',\n",
       " 23385: 'stoicism',\n",
       " 14899: \"guys'\",\n",
       " 52080: \"beroemd'\",\n",
       " 6706: 'butcher',\n",
       " 40860: \"melfi's\",\n",
       " 30626: 'aargh',\n",
       " 20602: 'playhouse',\n",
       " 11311: 'wickedly',\n",
       " 1183: 'fit',\n",
       " 52081: 'labratory',\n",
       " 40862: 'lifeline',\n",
       " 1930: 'screaming',\n",
       " 4290: 'fix',\n",
       " 52082: 'cineliterate',\n",
       " 52083: 'fic',\n",
       " 52084: 'fia',\n",
       " 34717: 'fig',\n",
       " 52085: 'fmvs',\n",
       " 52086: 'fie',\n",
       " 52087: 'reentered',\n",
       " 30577: 'fin',\n",
       " 52088: 'doctresses',\n",
       " 52089: 'fil',\n",
       " 12609: 'zucker',\n",
       " 31934: 'ached',\n",
       " 52091: 'counsil',\n",
       " 52092: 'paterfamilias',\n",
       " 13888: 'songwriter',\n",
       " 34718: 'shivam',\n",
       " 9657: 'hurting',\n",
       " 302: 'effects',\n",
       " 52093: 'slauther',\n",
       " 52094: \"'flame'\",\n",
       " 52095: 'sommerset',\n",
       " 52096: 'interwhined',\n",
       " 27641: 'whacking',\n",
       " 52097: 'bartok',\n",
       " 8778: 'barton',\n",
       " 21912: 'frewer',\n",
       " 52098: \"fi'\",\n",
       " 6195: 'ingrid',\n",
       " 30578: 'stribor',\n",
       " 52099: 'approporiately',\n",
       " 52100: 'wobblyhand',\n",
       " 52101: 'tantalisingly',\n",
       " 52102: 'ankylosaurus',\n",
       " 17637: 'parasites',\n",
       " 52103: 'childen',\n",
       " 52104: \"jenkins'\",\n",
       " 52105: 'metafiction',\n",
       " 17638: 'golem',\n",
       " 40863: 'indiscretion',\n",
       " 23386: \"reeves'\",\n",
       " 57784: \"inamorata's\",\n",
       " 52107: 'brittannica',\n",
       " 7919: 'adapt',\n",
       " 30579: \"russo's\",\n",
       " 48249: 'guitarists',\n",
       " 10556: 'abbott',\n",
       " 40864: 'abbots',\n",
       " 17652: 'lanisha',\n",
       " 40866: 'magickal',\n",
       " 52108: 'mattter',\n",
       " 52109: \"'willy\",\n",
       " 34719: 'pumpkins',\n",
       " 52110: 'stuntpeople',\n",
       " 30580: 'estimate',\n",
       " 40867: 'ugghhh',\n",
       " 11312: 'gameplay',\n",
       " 52111: \"wern't\",\n",
       " 40868: \"n'sync\",\n",
       " 16120: 'sickeningly',\n",
       " 40869: 'chiara',\n",
       " 4014: 'disturbed',\n",
       " 40870: 'portmanteau',\n",
       " 52112: 'ineffectively',\n",
       " 82146: \"duchonvey's\",\n",
       " 37522: \"nasty'\",\n",
       " 1288: 'purpose',\n",
       " 52115: 'lazers',\n",
       " 28108: 'lightened',\n",
       " 52116: 'kaliganj',\n",
       " 52117: 'popularism',\n",
       " 18514: \"damme's\",\n",
       " 30581: 'stylistics',\n",
       " 52118: 'mindgaming',\n",
       " 46452: 'spoilerish',\n",
       " 52120: \"'corny'\",\n",
       " 34721: 'boerner',\n",
       " 6795: 'olds',\n",
       " 52121: 'bakelite',\n",
       " 27642: 'renovated',\n",
       " 27643: 'forrester',\n",
       " 52122: \"lumiere's\",\n",
       " 52027: 'gaskets',\n",
       " 887: 'needed',\n",
       " 34722: 'smight',\n",
       " 1300: 'master',\n",
       " 25908: \"edie's\",\n",
       " 40871: 'seeber',\n",
       " 52123: 'hiya',\n",
       " 52124: 'fuzziness',\n",
       " 14900: 'genesis',\n",
       " 12610: 'rewards',\n",
       " 30582: 'enthrall',\n",
       " 40872: \"'about\",\n",
       " 52125: \"recollection's\",\n",
       " 11042: 'mutilated',\n",
       " 52126: 'fatherlands',\n",
       " 52127: \"fischer's\",\n",
       " 5402: 'positively',\n",
       " 34708: '270',\n",
       " 34723: 'ahmed',\n",
       " 9839: 'zatoichi',\n",
       " 13889: 'bannister',\n",
       " 52130: 'anniversaries',\n",
       " 30583: \"helm's\",\n",
       " 52131: \"'work'\",\n",
       " 34724: 'exclaimed',\n",
       " 52132: \"'unfunny'\",\n",
       " 52032: '274',\n",
       " 547: 'feeling',\n",
       " 52134: \"wanda's\",\n",
       " 33269: 'dolan',\n",
       " 52136: '278',\n",
       " 52137: 'peacoat',\n",
       " 40873: 'brawny',\n",
       " 40874: 'mishra',\n",
       " 40875: 'worlders',\n",
       " 52138: 'protags',\n",
       " 52139: 'skullcap',\n",
       " 57599: 'dastagir',\n",
       " 5625: 'affairs',\n",
       " 7802: 'wholesome',\n",
       " 52140: 'hymen',\n",
       " 25249: 'paramedics',\n",
       " 52141: 'unpersons',\n",
       " 52142: 'heavyarms',\n",
       " 52143: 'affaire',\n",
       " 52144: 'coulisses',\n",
       " 40876: 'hymer',\n",
       " 52145: 'kremlin',\n",
       " 30584: 'shipments',\n",
       " 52146: 'pixilated',\n",
       " 30585: \"'00s\",\n",
       " 18515: 'diminishing',\n",
       " 1360: 'cinematic',\n",
       " 14901: 'resonates',\n",
       " 40877: 'simplify',\n",
       " 40878: \"nature'\",\n",
       " 40879: 'temptresses',\n",
       " 16825: 'reverence',\n",
       " 19505: 'resonated',\n",
       " 34725: 'dailey',\n",
       " 52147: '2\\x85',\n",
       " 27644: 'treize',\n",
       " 52148: 'majo',\n",
       " 21913: 'kiya',\n",
       " 52149: 'woolnough',\n",
       " 39800: 'thanatos',\n",
       " 35734: 'sandoval',\n",
       " 40882: 'dorama',\n",
       " 52150: \"o'shaughnessy\",\n",
       " 4991: 'tech',\n",
       " 32021: 'fugitives',\n",
       " 30586: 'teck',\n",
       " 76128: \"'e'\",\n",
       " 40884: 'doesn’t',\n",
       " 52152: 'purged',\n",
       " 660: 'saying',\n",
       " 41098: \"martians'\",\n",
       " 23421: 'norliss',\n",
       " 27645: 'dickey',\n",
       " 52155: 'dicker',\n",
       " 52156: \"'sependipity\",\n",
       " 8425: 'padded',\n",
       " 57795: 'ordell',\n",
       " 40885: \"sturges'\",\n",
       " 52157: 'independentcritics',\n",
       " 5748: 'tempted',\n",
       " 34727: \"atkinson's\",\n",
       " 25250: 'hounded',\n",
       " 52158: 'apace',\n",
       " 15497: 'clicked',\n",
       " 30587: \"'humor'\",\n",
       " 17180: \"martino's\",\n",
       " 52159: \"'supporting\",\n",
       " 52035: 'warmongering',\n",
       " 34728: \"zemeckis's\",\n",
       " 21914: 'lube',\n",
       " 52160: 'shocky',\n",
       " 7479: 'plate',\n",
       " 40886: 'plata',\n",
       " 40887: 'sturgess',\n",
       " 40888: \"nerds'\",\n",
       " 20603: 'plato',\n",
       " 34729: 'plath',\n",
       " 40889: 'platt',\n",
       " 52162: 'mcnab',\n",
       " 27646: 'clumsiness',\n",
       " 3902: 'altogether',\n",
       " 42587: 'massacring',\n",
       " 52163: 'bicenntinial',\n",
       " 40890: 'skaal',\n",
       " 14363: 'droning',\n",
       " 8779: 'lds',\n",
       " 21915: 'jaguar',\n",
       " 34730: \"cale's\",\n",
       " 1780: 'nicely',\n",
       " 4591: 'mummy',\n",
       " 18516: \"lot's\",\n",
       " 10089: 'patch',\n",
       " 50205: 'kerkhof',\n",
       " 52164: \"leader's\",\n",
       " 27647: \"'movie\",\n",
       " 52165: 'uncomfirmed',\n",
       " 40891: 'heirloom',\n",
       " 47363: 'wrangle',\n",
       " 52166: 'emotion\\x85',\n",
       " 52167: \"'stargate'\",\n",
       " 40892: 'pinoy',\n",
       " 40893: 'conchatta',\n",
       " 41131: 'broeke',\n",
       " 40894: 'advisedly',\n",
       " 17639: \"barker's\",\n",
       " 52169: 'descours',\n",
       " 775: 'lots',\n",
       " 9262: 'lotr',\n",
       " 9882: 'irs',\n",
       " 52170: 'lott',\n",
       " 40895: 'xvi',\n",
       " 34731: 'irk',\n",
       " 52171: 'irl',\n",
       " 6890: 'ira',\n",
       " 21916: 'belzer',\n",
       " 52172: 'irc',\n",
       " 27648: 'ire',\n",
       " 40896: 'requisites',\n",
       " 7696: 'discipline',\n",
       " 52964: 'lyoko',\n",
       " 11313: 'extend',\n",
       " 876: 'nature',\n",
       " 52173: \"'dickie'\",\n",
       " 40897: 'optimist',\n",
       " 30589: 'lapping',\n",
       " 3903: 'superficial',\n",
       " 52174: 'vestment',\n",
       " 2826: 'extent',\n",
       " 52175: 'tendons',\n",
       " 52176: \"heller's\",\n",
       " 52177: 'quagmires',\n",
       " 52178: 'miyako',\n",
       " 20604: 'moocow',\n",
       " 52179: \"coles'\",\n",
       " 40898: 'lookit',\n",
       " 52180: 'ravenously',\n",
       " 40899: 'levitating',\n",
       " 52181: 'perfunctorily',\n",
       " 30590: 'lookin',\n",
       " 40901: \"lot'\",\n",
       " 52182: 'lookie',\n",
       " 34873: 'fearlessly',\n",
       " 52184: 'libyan',\n",
       " 40902: 'fondles',\n",
       " 35717: 'gopher',\n",
       " 40904: 'wearying',\n",
       " 52185: \"nz's\",\n",
       " 27649: 'minuses',\n",
       " 52186: 'puposelessly',\n",
       " 52187: 'shandling',\n",
       " 31271: 'decapitates',\n",
       " 11932: 'humming',\n",
       " 40905: \"'nother\",\n",
       " 21917: 'smackdown',\n",
       " 30591: 'underdone',\n",
       " 40906: 'frf',\n",
       " 52188: 'triviality',\n",
       " 25251: 'fro',\n",
       " 8780: 'bothers',\n",
       " 52189: \"'kensington\",\n",
       " 76: 'much',\n",
       " 34733: 'muco',\n",
       " 22618: 'wiseguy',\n",
       " 27651: \"richie's\",\n",
       " 40907: 'tonino',\n",
       " 52190: 'unleavened',\n",
       " 11590: 'fry',\n",
       " 40908: \"'tv'\",\n",
       " 40909: 'toning',\n",
       " 14364: 'obese',\n",
       " 30592: 'sensationalized',\n",
       " 40910: 'spiv',\n",
       " 6262: 'spit',\n",
       " 7367: 'arkin',\n",
       " 21918: 'charleton',\n",
       " 16826: 'jeon',\n",
       " 21919: 'boardroom',\n",
       " 4992: 'doubts',\n",
       " 3087: 'spin',\n",
       " 53086: 'hepo',\n",
       " 27652: 'wildcat',\n",
       " 10587: 'venoms',\n",
       " 52194: 'misconstrues',\n",
       " 18517: 'mesmerising',\n",
       " 40911: 'misconstrued',\n",
       " 52195: 'rescinds',\n",
       " 52196: 'prostrate',\n",
       " 40912: 'majid',\n",
       " 16482: 'climbed',\n",
       " 34734: 'canoeing',\n",
       " 52198: 'majin',\n",
       " 57807: 'animie',\n",
       " 40913: 'sylke',\n",
       " 14902: 'conditioned',\n",
       " 40914: 'waddell',\n",
       " 52199: '3\\x85',\n",
       " 41191: 'hyperdrive',\n",
       " 34735: 'conditioner',\n",
       " 53156: 'bricklayer',\n",
       " 2579: 'hong',\n",
       " 52201: 'memoriam',\n",
       " 30595: 'inventively',\n",
       " 25252: \"levant's\",\n",
       " 20641: 'portobello',\n",
       " 52203: 'remand',\n",
       " 19507: 'mummified',\n",
       " 27653: 'honk',\n",
       " 19508: 'spews',\n",
       " 40915: 'visitations',\n",
       " 52204: 'mummifies',\n",
       " 25253: 'cavanaugh',\n",
       " 23388: 'zeon',\n",
       " 40916: \"jungle's\",\n",
       " 34736: 'viertel',\n",
       " 27654: 'frenchmen',\n",
       " 52205: 'torpedoes',\n",
       " 52206: 'schlessinger',\n",
       " 34737: 'torpedoed',\n",
       " 69879: 'blister',\n",
       " 52207: 'cinefest',\n",
       " 34738: 'furlough',\n",
       " 52208: 'mainsequence',\n",
       " 40917: 'mentors',\n",
       " 9097: 'academic',\n",
       " 20605: 'stillness',\n",
       " 40918: 'academia',\n",
       " 52209: 'lonelier',\n",
       " 52210: 'nibby',\n",
       " 52211: \"losers'\",\n",
       " 40919: 'cineastes',\n",
       " 4452: 'corporate',\n",
       " 40920: 'massaging',\n",
       " 30596: 'bellow',\n",
       " 19509: 'absurdities',\n",
       " 53244: 'expetations',\n",
       " 40921: 'nyfiken',\n",
       " 75641: 'mehras',\n",
       " 52212: 'lasse',\n",
       " 52213: 'visability',\n",
       " 33949: 'militarily',\n",
       " 52214: \"elder'\",\n",
       " 19026: 'gainsbourg',\n",
       " 20606: 'hah',\n",
       " 13423: 'hai',\n",
       " 34739: 'haj',\n",
       " 25254: 'hak',\n",
       " 4314: 'hal',\n",
       " 4895: 'ham',\n",
       " 53262: 'duffer',\n",
       " 52216: 'haa',\n",
       " 69: 'had',\n",
       " 11933: 'advancement',\n",
       " 16828: 'hag',\n",
       " 25255: \"hand'\",\n",
       " 13424: 'hay',\n",
       " 20607: 'mcnamara',\n",
       " 52217: \"mozart's\",\n",
       " 30734: 'duffel',\n",
       " 30597: 'haq',\n",
       " 13890: 'har',\n",
       " 47: 'has',\n",
       " 2404: 'hat',\n",
       " 40922: 'hav',\n",
       " 30598: 'haw',\n",
       " 52218: 'figtings',\n",
       " 15498: 'elders',\n",
       " 52219: 'underpanted',\n",
       " 52220: 'pninson',\n",
       " 27655: 'unequivocally',\n",
       " 23676: \"barbara's\",\n",
       " 52222: \"bello'\",\n",
       " 13000: 'indicative',\n",
       " 40923: 'yawnfest',\n",
       " 52223: 'hexploitation',\n",
       " 52224: \"loder's\",\n",
       " 27656: 'sleuthing',\n",
       " 32625: \"justin's\",\n",
       " 52225: \"'ball\",\n",
       " 52226: \"'summer\",\n",
       " 34938: \"'demons'\",\n",
       " 52228: \"mormon's\",\n",
       " 34740: \"laughton's\",\n",
       " 52229: 'debell',\n",
       " 39727: 'shipyard',\n",
       " 30600: 'unabashedly',\n",
       " 40404: 'disks',\n",
       " 2293: 'crowd',\n",
       " 10090: 'crowe',\n",
       " 56437: \"vancouver's\",\n",
       " 34741: 'mosques',\n",
       " 6630: 'crown',\n",
       " 52230: 'culpas',\n",
       " 27657: 'crows',\n",
       " 53347: 'surrell',\n",
       " 52232: 'flowless',\n",
       " 52233: 'sheirk',\n",
       " 40926: \"'three\",\n",
       " 52234: \"peterson'\",\n",
       " 52235: 'ooverall',\n",
       " 40927: 'perchance',\n",
       " 1324: 'bottom',\n",
       " 53366: 'chabert',\n",
       " 52236: 'sneha',\n",
       " 13891: 'inhuman',\n",
       " 52237: 'ichii',\n",
       " 52238: 'ursla',\n",
       " 30601: 'completly',\n",
       " 40928: 'moviedom',\n",
       " 52239: 'raddick',\n",
       " 51998: 'brundage',\n",
       " 40929: 'brigades',\n",
       " 1184: 'starring',\n",
       " 52240: \"'goal'\",\n",
       " 52241: 'caskets',\n",
       " 52242: 'willcock',\n",
       " 52243: \"threesome's\",\n",
       " 52244: \"mosque'\",\n",
       " 52245: \"cover's\",\n",
       " 17640: 'spaceships',\n",
       " 40930: 'anomalous',\n",
       " 27658: 'ptsd',\n",
       " 52246: 'shirdan',\n",
       " 21965: 'obscenity',\n",
       " 30602: 'lemmings',\n",
       " 30603: 'duccio',\n",
       " 52247: \"levene's\",\n",
       " 52248: \"'gorby'\",\n",
       " 25258: \"teenager's\",\n",
       " 5343: 'marshall',\n",
       " 9098: 'honeymoon',\n",
       " 3234: 'shoots',\n",
       " 12261: 'despised',\n",
       " 52249: 'okabasho',\n",
       " 8292: 'fabric',\n",
       " 18518: 'cannavale',\n",
       " 3540: 'raped',\n",
       " 52250: \"tutt's\",\n",
       " 17641: 'grasping',\n",
       " 18519: 'despises',\n",
       " 40931: \"thief's\",\n",
       " 8929: 'rapes',\n",
       " 52251: 'raper',\n",
       " 27659: \"eyre'\",\n",
       " 52252: 'walchek',\n",
       " 23389: \"elmo's\",\n",
       " 40932: 'perfumes',\n",
       " 21921: 'spurting',\n",
       " 52253: \"exposition'\\x85\",\n",
       " 52254: 'denoting',\n",
       " 34743: 'thesaurus',\n",
       " 40933: \"shoot'\",\n",
       " 49762: 'bonejack',\n",
       " 52256: 'simpsonian',\n",
       " 30604: 'hebetude',\n",
       " 34744: \"hallow's\",\n",
       " 52257: 'desperation\\x85',\n",
       " 34745: 'incinerator',\n",
       " 10311: 'congratulations',\n",
       " 52258: 'humbled',\n",
       " 5927: \"else's\",\n",
       " 40848: 'trelkovski',\n",
       " 52259: \"rape'\",\n",
       " 59389: \"'chapters'\",\n",
       " 52260: '1600s',\n",
       " 7256: 'martian',\n",
       " 25259: 'nicest',\n",
       " 52262: 'eyred',\n",
       " 9460: 'passenger',\n",
       " 6044: 'disgrace',\n",
       " 52263: 'moderne',\n",
       " 5123: 'barrymore',\n",
       " 52264: 'yankovich',\n",
       " 40934: 'moderns',\n",
       " 52265: 'studliest',\n",
       " 52266: 'bedsheet',\n",
       " 14903: 'decapitation',\n",
       " 52267: 'slurring',\n",
       " 52268: \"'nunsploitation'\",\n",
       " 34746: \"'character'\",\n",
       " 9883: 'cambodia',\n",
       " 52269: 'rebelious',\n",
       " 27660: 'pasadena',\n",
       " 40935: 'crowne',\n",
       " 52270: \"'bedchamber\",\n",
       " 52271: 'conjectural',\n",
       " 52272: 'appologize',\n",
       " 52273: 'halfassing',\n",
       " 57819: 'paycheque',\n",
       " 20609: 'palms',\n",
       " 52274: \"'islands\",\n",
       " 40936: 'hawked',\n",
       " 21922: 'palme',\n",
       " 40937: 'conservatively',\n",
       " 64010: 'larp',\n",
       " 5561: 'palma',\n",
       " 21923: 'smelling',\n",
       " 13001: 'aragorn',\n",
       " 52275: 'hawker',\n",
       " 52276: 'hawkes',\n",
       " 3978: 'explosions',\n",
       " 8062: 'loren',\n",
       " 52277: \"pyle's\",\n",
       " 6707: 'shootout',\n",
       " 18520: \"mike's\",\n",
       " 52278: \"driscoll's\",\n",
       " 40938: 'cogsworth',\n",
       " 52279: \"britian's\",\n",
       " 34747: 'childs',\n",
       " 52280: \"portrait's\",\n",
       " 3629: 'chain',\n",
       " 2500: 'whoever',\n",
       " 52281: 'puttered',\n",
       " 52282: 'childe',\n",
       " 52283: 'maywether',\n",
       " 3039: 'chair',\n",
       " 52284: \"rance's\",\n",
       " 34748: 'machu',\n",
       " 4520: 'ballet',\n",
       " 34749: 'grapples',\n",
       " 76155: 'summerize',\n",
       " 30606: 'freelance',\n",
       " 52286: \"andrea's\",\n",
       " 52287: '\\x91very',\n",
       " 45882: 'coolidge',\n",
       " 18521: 'mache',\n",
       " 52288: 'balled',\n",
       " 40940: 'grappled',\n",
       " 18522: 'macha',\n",
       " 21924: 'underlining',\n",
       " 5626: 'macho',\n",
       " 19510: 'oversight',\n",
       " 25260: 'machi',\n",
       " 11314: 'verbally',\n",
       " 21925: 'tenacious',\n",
       " 40941: 'windshields',\n",
       " 18560: 'paychecks',\n",
       " 3399: 'jerk',\n",
       " 11934: \"good'\",\n",
       " 34751: 'prancer',\n",
       " 21926: 'prances',\n",
       " 52289: 'olympus',\n",
       " 21927: 'lark',\n",
       " 10788: 'embark',\n",
       " 7368: 'gloomy',\n",
       " 52290: 'jehaan',\n",
       " 52291: 'turaqui',\n",
       " 20610: \"child'\",\n",
       " 2897: 'locked',\n",
       " 52292: 'pranced',\n",
       " 2591: 'exact',\n",
       " 52293: 'unattuned',\n",
       " 786: 'minute',\n",
       " 16121: 'skewed',\n",
       " 40943: 'hodgins',\n",
       " 34752: 'skewer',\n",
       " 52294: 'think\\x85',\n",
       " 38768: 'rosenstein',\n",
       " 52295: 'helmit',\n",
       " 34753: 'wrestlemanias',\n",
       " 16829: 'hindered',\n",
       " 30607: \"martha's\",\n",
       " 52296: 'cheree',\n",
       " 52297: \"pluckin'\",\n",
       " 40944: 'ogles',\n",
       " 11935: 'heavyweight',\n",
       " 82193: 'aada',\n",
       " 11315: 'chopping',\n",
       " 61537: 'strongboy',\n",
       " 41345: 'hegemonic',\n",
       " 40945: 'adorns',\n",
       " 41349: 'xxth',\n",
       " 34754: 'nobuhiro',\n",
       " 52301: 'capitães',\n",
       " 52302: 'kavogianni',\n",
       " 13425: 'antwerp',\n",
       " 6541: 'celebrated',\n",
       " 52303: 'roarke',\n",
       " 40946: 'baggins',\n",
       " 31273: 'cheeseburgers',\n",
       " 52304: 'matras',\n",
       " 52305: \"nineties'\",\n",
       " 52306: \"'craig'\",\n",
       " 13002: 'celebrates',\n",
       " 3386: 'unintentionally',\n",
       " 14365: 'drafted',\n",
       " 52307: 'climby',\n",
       " 52308: '303',\n",
       " 18523: 'oldies',\n",
       " 9099: 'climbs',\n",
       " 9658: 'honour',\n",
       " 34755: 'plucking',\n",
       " 30077: '305',\n",
       " 5517: 'address',\n",
       " 40947: 'menjou',\n",
       " 42595: \"'freak'\",\n",
       " 19511: 'dwindling',\n",
       " 9461: 'benson',\n",
       " 52310: 'white’s',\n",
       " 40948: 'shamelessness',\n",
       " 21928: 'impacted',\n",
       " 52311: 'upatz',\n",
       " 3843: 'cusack',\n",
       " 37570: \"flavia's\",\n",
       " 52312: 'effette',\n",
       " 34756: 'influx',\n",
       " 52313: 'boooooooo',\n",
       " 52314: 'dimitrova',\n",
       " 13426: 'houseman',\n",
       " 25262: 'bigas',\n",
       " 52315: 'boylen',\n",
       " 52316: 'phillipenes',\n",
       " 40949: 'fakery',\n",
       " 27661: \"grandpa's\",\n",
       " 27662: 'darnell',\n",
       " 19512: 'undergone',\n",
       " 52318: 'handbags',\n",
       " 21929: 'perished',\n",
       " 37781: 'pooped',\n",
       " 27663: 'vigour',\n",
       " 3630: 'opposed',\n",
       " 52319: 'etude',\n",
       " 11802: \"caine's\",\n",
       " 52320: 'doozers',\n",
       " 34757: 'photojournals',\n",
       " 52321: 'perishes',\n",
       " 34758: 'constrains',\n",
       " 40951: 'migenes',\n",
       " 30608: 'consoled',\n",
       " 16830: 'alastair',\n",
       " 52322: 'wvs',\n",
       " 52323: 'ooooooh',\n",
       " 34759: 'approving',\n",
       " 40952: 'consoles',\n",
       " 52067: 'disparagement',\n",
       " 52325: 'futureistic',\n",
       " 52326: 'rebounding',\n",
       " 52327: \"'date\",\n",
       " 52328: 'gregoire',\n",
       " 21930: 'rutherford',\n",
       " 34760: 'americanised',\n",
       " 82199: 'novikov',\n",
       " 1045: 'following',\n",
       " 34761: 'munroe',\n",
       " 52329: \"morita'\",\n",
       " 52330: 'christenssen',\n",
       " 23109: 'oatmeal',\n",
       " 25263: 'fossey',\n",
       " 40953: 'livered',\n",
       " 13003: 'listens',\n",
       " 76167: \"'marci\",\n",
       " 52333: \"otis's\",\n",
       " 23390: 'thanking',\n",
       " 16022: 'maude',\n",
       " 34762: 'extensions',\n",
       " 52335: 'ameteurish',\n",
       " 52336: \"commender's\",\n",
       " 27664: 'agricultural',\n",
       " 4521: 'convincingly',\n",
       " 17642: 'fueled',\n",
       " 54017: 'mahattan',\n",
       " 40955: \"paris's\",\n",
       " 52339: 'vulkan',\n",
       " 52340: 'stapes',\n",
       " 52341: 'odysessy',\n",
       " 12262: 'harmon',\n",
       " 4255: 'surfing',\n",
       " 23497: 'halloran',\n",
       " 49583: 'unbelieveably',\n",
       " 52342: \"'offed'\",\n",
       " 30610: 'quadrant',\n",
       " 19513: 'inhabiting',\n",
       " 34763: 'nebbish',\n",
       " 40956: 'forebears',\n",
       " 34764: 'skirmish',\n",
       " 52343: 'ocassionally',\n",
       " 52344: \"'resist\",\n",
       " 21931: 'impactful',\n",
       " 52345: 'spicier',\n",
       " 40957: 'touristy',\n",
       " 52346: \"'football'\",\n",
       " 40958: 'webpage',\n",
       " 52348: 'exurbia',\n",
       " 52349: 'jucier',\n",
       " 14904: 'professors',\n",
       " 34765: 'structuring',\n",
       " 30611: 'jig',\n",
       " 40959: 'overlord',\n",
       " 25264: 'disconnect',\n",
       " 82204: 'sniffle',\n",
       " 40960: 'slimeball',\n",
       " 40961: 'jia',\n",
       " 16831: 'milked',\n",
       " 40962: 'banjoes',\n",
       " 1240: 'jim',\n",
       " 52351: 'workforces',\n",
       " 52352: 'jip',\n",
       " 52353: 'rotweiller',\n",
       " 34766: 'mundaneness',\n",
       " 52354: \"'ninja'\",\n",
       " 11043: \"dead'\",\n",
       " 40963: \"cipriani's\",\n",
       " 20611: 'modestly',\n",
       " 52355: \"professor'\",\n",
       " 40964: 'shacked',\n",
       " 34767: 'bashful',\n",
       " 23391: 'sorter',\n",
       " 16123: 'overpowering',\n",
       " 18524: 'workmanlike',\n",
       " 27665: 'henpecked',\n",
       " 18525: 'sorted',\n",
       " 52357: \"jōb's\",\n",
       " 52358: \"'always\",\n",
       " 34768: \"'baptists\",\n",
       " 52359: 'dreamcatchers',\n",
       " 52360: \"'silence'\",\n",
       " 21932: 'hickory',\n",
       " 52361: 'fun\\x97yet',\n",
       " 52362: 'breakumentary',\n",
       " 15499: 'didn',\n",
       " 52363: 'didi',\n",
       " 52364: 'pealing',\n",
       " 40965: 'dispite',\n",
       " 25265: \"italy's\",\n",
       " 21933: 'instability',\n",
       " 6542: 'quarter',\n",
       " 12611: 'quartet',\n",
       " 52365: 'padmé',\n",
       " 52366: \"'bleedmedry\",\n",
       " 52367: 'pahalniuk',\n",
       " 52368: 'honduras',\n",
       " 10789: 'bursting',\n",
       " 41468: \"pablo's\",\n",
       " 52370: 'irremediably',\n",
       " 40966: 'presages',\n",
       " 57835: 'bowlegged',\n",
       " 65186: 'dalip',\n",
       " 6263: 'entering',\n",
       " 76175: 'newsradio',\n",
       " 54153: 'presaged',\n",
       " 27666: \"giallo's\",\n",
       " 40967: 'bouyant',\n",
       " 52371: 'amerterish',\n",
       " 18526: 'rajni',\n",
       " 30613: 'leeves',\n",
       " 34770: 'macauley',\n",
       " 615: 'seriously',\n",
       " 52372: 'sugercoma',\n",
       " 52373: 'grimstead',\n",
       " 52374: \"'fairy'\",\n",
       " 30614: 'zenda',\n",
       " 52375: \"'twins'\",\n",
       " 17643: 'realisation',\n",
       " 27667: 'highsmith',\n",
       " 7820: 'raunchy',\n",
       " 40968: 'incentives',\n",
       " 52377: 'flatson',\n",
       " 35100: 'snooker',\n",
       " 16832: 'crazies',\n",
       " 14905: 'crazier',\n",
       " 7097: 'grandma',\n",
       " 52378: 'napunsaktha',\n",
       " 30615: 'workmanship',\n",
       " 52379: 'reisner',\n",
       " 61309: \"sanford's\",\n",
       " 52380: '\\x91doña',\n",
       " 6111: 'modest',\n",
       " 19156: \"everything's\",\n",
       " 40969: 'hamer',\n",
       " 52382: \"couldn't'\",\n",
       " 13004: 'quibble',\n",
       " 52383: 'socking',\n",
       " 21934: 'tingler',\n",
       " 52384: 'gutman',\n",
       " 40970: 'lachlan',\n",
       " 52385: 'tableaus',\n",
       " 52386: 'headbanger',\n",
       " 2850: 'spoken',\n",
       " 34771: 'cerebrally',\n",
       " 23493: \"'road\",\n",
       " 21935: 'tableaux',\n",
       " 40971: \"proust's\",\n",
       " 40972: 'periodical',\n",
       " 52388: \"shoveller's\",\n",
       " 25266: 'tamara',\n",
       " 17644: 'affords',\n",
       " 3252: 'concert',\n",
       " 87958: \"yara's\",\n",
       " 52389: 'someome',\n",
       " 8427: 'lingering',\n",
       " 41514: \"abraham's\",\n",
       " 34772: 'beesley',\n",
       " 34773: 'cherbourg',\n",
       " 28627: 'kagan',\n",
       " 9100: 'snatch',\n",
       " 9263: \"miyazaki's\",\n",
       " 25267: 'absorbs',\n",
       " 40973: \"koltai's\",\n",
       " 64030: 'tingled',\n",
       " 19514: 'crossroads',\n",
       " 16124: 'rehab',\n",
       " 52392: 'falworth',\n",
       " 52393: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dictionnaire de mot\n",
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 218, 189)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Liste d'indice vers le dictionnaire\n",
    "len(X_train),len(X_train[0]),len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Hfl42LGCugWB",
    "outputId": "f4a31722-3ea8-44ef-e021-3f35a693bbfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(y_train): <class 'numpy.ndarray'>\n",
      "y_train.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"type(y_train):\", type(y_train))\n",
    "print(\"y_train.shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iVw65PNNuobX",
    "outputId": "aabb9be8-21b4-4945-c842-9f3c164dd1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (25000,)\n",
      "y_test.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-04cea84bf598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V18OA7oQNH3c"
   },
   "source": [
    "## Data processing\n",
    "\n",
    "Sequences (represented as a list of values) in ```X_train``` represent the reviews.\n",
    "They can have different length.\n",
    "To train the network we should modify them so that they all have the same length.\n",
    "We do this by:\n",
    "- truncating the ones that are too long\n",
    "- padding-with-zero them the ones that are too short.\n",
    "\n",
    "This is obtained using ```sequence.pad_sequences``` of keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "JhmiHsOGoRwT",
    "outputId": "a72a2840-07b3-4186-dcd8-825abda5ef08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train[0]): 100\n",
      "len(X_train[1]): 100\n",
      "X_train[0]: [   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      "    2   16]\n"
     ]
    }
   ],
   "source": [
    "# truncate and pad input sequences\n",
    "\n",
    "# CODE-RNN1-1\n",
    "\n",
    "# --- START CODE HERE\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, value=0.0, padding=\"post\", truncating=\"post\")\n",
    "#Padding post : on rajoute les 0 après la fin de la phrase\n",
    "#truncating post : on coupe la fin de la phrase et non le début (pre)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, value=0.0, padding=\"post\", truncating=\"post\")\n",
    "# --- END CODE HERE\n",
    "\n",
    "print(\"len(X_train[0]):\", len(X_train[0]))\n",
    "print(\"len(X_train[1]):\", len(X_train[1]))\n",
    "print(\"X_train[0]:\", X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the first training sequence: len(X_train[0]): 100\n",
      "length of the second training sequence: len(X_train[0]): 100\n",
      "maximum length of a training sequence: 100\n"
     ]
    }
   ],
   "source": [
    "test = sequence.pad_sequences(X_train, maxlen=max_review_length, value=0.0, padding=\"post\", truncating=\"post\")\n",
    "#Padding post : on rajoute les 0 après la fin de la phrase\n",
    "#truncating post : on coupe la fin de la phrase et non le début (pre)\n",
    "print(\"length of the first training sequence: len(X_train[0]):\",len(test[0]))\n",
    "print(\"length of the second training sequence: len(X_train[0]):\",len(test[5]))\n",
    "len_list = [len(x) for x in test]\n",
    "print(\"maximum length of a training sequence:\", max(len_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,  778,  128,   74,   12,  630,  163,   15,    4, 1766,    2,\n",
       "        1051,    2,   32,   85,  156,   45,   40,  148,  139,  121,  664,\n",
       "         665,   10,   10, 1361,  173,    4,  749,    2,   16, 3804,    8,\n",
       "           4,  226,   65,   12,   43,  127,   24,    2,   10,   10,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]),\n",
       " array([1415,   33,    6,   22,   12]),\n",
       " [1, 14, 22, 16, 43])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[5],test[0][:5],X_train[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlrDTuk5K65Q"
   },
   "source": [
    "## First model\n",
    "\n",
    "In the first model, we will simply \n",
    "- learn a word embedding  (```Embedding``` layer in keras) and apply it to each of item of a sequence, \n",
    "  -  in keras, embedding is not a matrix going from one-hot-encoding to embedding, it is a layer that goes from index-in-word-dictionary to embedding\n",
    "  - the embedding goes from ```top_words``` dimensions to  ```embedding_vector_length``` dimensions\n",
    "- average the embedding obtained for each word of a sequence over all words of the sequence (you should use ```K.mean``` and ```Lambda``` from the keras backend)\n",
    "- apply a fully connected (```Dense``` layer in keras) which output activation is a sigmoid() predicting the 0 or 1 rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufW00TGcs3Jj"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "zspaUptgtW9l",
    "outputId": "9f627c1b-1814-4ce3-83a9-4ff4ef99ec3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "\n",
    "# CODE-RNN1-2\n",
    "# --- START CODE HERE\n",
    "model = Sequential()\n",
    "input_dim=top_words#Taille du dictionnaire\n",
    "output_dim=embedding_vector_length#Taille de vect embedding\n",
    "model.add(Embedding(input_dim, output_dim, input_length=100))\n",
    "\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1)))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "output_array = model.predict(X_train)\n",
    "# --- END CODE HERE\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "pFXz4AS6tawQ",
    "outputId": "403fc94e-bba1-4587-da8f-0578817e5aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 2s 3ms/step - loss: 0.6724 - accuracy: 0.6285 - val_loss: 0.5821 - val_accuracy: 0.7421\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.5383 - accuracy: 0.7794 - val_loss: 0.4801 - val_accuracy: 0.7944\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.4449 - accuracy: 0.8148 - val_loss: 0.4327 - val_accuracy: 0.8066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x282861b8f48>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBqyzLJRUIsC"
   },
   "source": [
    "## Results\n",
    "\n",
    "After only 3 epochs, you should obtain an accuracy around 84% for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.66%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the trained embedding to find equivalence between words\n",
    "\n",
    "Since the embedding is part of the models, we can look at the trained embedding matrix $E$ and use it to get the most similar words (according to the trained matrix $E$) in the dictionary.\n",
    "Use the weights of the ```Embedding``` layer to find the most similar words to ```amazing```. We will use an Euclidean distance for that.\n",
    "- Retrieve the weights of the ```Embedding layer```\n",
    "- Get the position of ```amazing``` in the dictionary\n",
    "- Get the word-embedding of ```amazing```\n",
    "- Find (using Euclidean distance), the closest embedded-words to ```amazing```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantastic\n",
      "favorite\n",
      "perfect\n",
      "loved\n",
      "superb\n",
      "amazing\n",
      "best\n",
      "great\n",
      "wonderful\n",
      "excellent\n"
     ]
    }
   ],
   "source": [
    "# CODE-RNN1-3\n",
    "# --- START CODE HERE\n",
    "#Matrice poids de la 1er couche (embedding)\n",
    "W_emb=model.get_weights()[0]\n",
    "\n",
    "# Récupération de l'indice du mot amazing\n",
    "for key,val in id_to_word.items():\n",
    "    if val == \"amazing\":\n",
    "        amazing=key\n",
    "\n",
    "#Création du vecteur amazing\n",
    "my_word=np.zeros((1,5000))\n",
    "my_word[0][amazing]=1.0\n",
    "\n",
    "#Calcul du vecteur embedding du mot amazing\n",
    "amazing_emb = my_word@W_emb\n",
    "#Calcul inverse pour retrouver le lien sémantique de tous les mots du dictionnaire avec le sens sématique de amazing dans cet espace\n",
    "sol = (W_emb@amazing_emb.T).ravel()\n",
    "\n",
    "#On sort les 10 premiers indices et on affiche les mots associés\n",
    "for i in sol.argsort()[-10::]:\n",
    "    print(id_to_word[i])\n",
    "\n",
    "# --- END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK9e5Eo1Ks2a"
   },
   "source": [
    "## Second model\n",
    "\n",
    "In the second model, we will replace\n",
    "- average the obtained embedding over the sequence (use ```K.mean``` and ```Lambda```from keras backend)\n",
    "- by a RNN layer (more precisely an ```LSTM```) in a Many-To-One configuration with $n_a=100$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwoXuOqqVDOy"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "7dl-CSMKoViX",
    "outputId": "0010cba8-4783-4201-eddc-54591e50594e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25000, 1), array([0.50054574], dtype=float32))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "\n",
    "# CODE-RNN1-4\n",
    "# --- START CODE HERE\n",
    "model = Sequential()\n",
    "input_dim=top_words#Taille du dictionnaire\n",
    "output_dim=embedding_vector_length#Taille de vect embedding\n",
    "model.add(Embedding(input_dim, output_dim, input_length=100))\n",
    "\n",
    "model.add(LSTM(units=100))\n",
    "#units donne le nombre de neuronne dans la couche caché\n",
    "          \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "output_array = model.predict(X_train)\n",
    "# --- END CODE HERE\n",
    "\n",
    "print(model.summary())\n",
    "output_array.shape,output_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "colab_type": "code",
    "id": "-bp7PzX7oXtB",
    "outputId": "dfe50350-814f-4527-d33c-26a54f8d2254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 31s 77ms/step - loss: 0.5866 - accuracy: 0.6543 - val_loss: 0.4271 - val_accuracy: 0.8083\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3510 - accuracy: 0.8575 - val_loss: 0.4112 - val_accuracy: 0.8153\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3209 - accuracy: 0.8738 - val_loss: 0.4357 - val_accuracy: 0.8012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x282d6b94f08>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1LN_fjMWBHJ"
   },
   "source": [
    "## Results\n",
    "\n",
    "After only 3 epochs, you should obtain an accuracy around 88% for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RlMEKRbzoavm",
    "outputId": "e912c880-ca49-40ef-8015-8ec1e7698531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.12%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP_RNN_imbd.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
