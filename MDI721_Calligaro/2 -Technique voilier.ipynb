{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calligaro Nicolas\n",
    "## Création des données techniques des bateaux\n",
    "Readme :\\\n",
    "Ce jupyter se concentre sur récupérer les données technique et produire un DataFrame exploitable.\\\n",
    "Runner l'ensemble du jupyter. vérifier que la dernière cellule et ok\\\n",
    "la dernière cellule est le 'main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import mptools\n",
    "html = 'https://www.vendeeglobe.org'\n",
    "df_clas=pickle.Unpickler(open(\"df_clas.pkl\",'rb')).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_skippers(html):\n",
    "    \"\"\"\n",
    "    Récupère la liste de tous les url des skippers depuis le site de base.\n",
    "    \"\"\"\n",
    "    r = requests.get(f'{html}/fr')\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    url = []\n",
    "    for tag in soup.findAll('a', attrs={'class': 'skippers__slide'}):\n",
    "        url.append(tag.attrs['href'])\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_boat (html):\n",
    "    \"\"\"\n",
    "    On scrap la page web et \n",
    "    On produit un dictionnaire {détail tech : valeur}\n",
    "    \"\"\"\n",
    "    r = requests.get(html)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    dico={}\n",
    "    masque = \"[0-9]*,[0-9]*\"\n",
    "    masque2 = \"[0-9]+\"\n",
    "    #On va produire le champ skipper\n",
    "    #Il a été vu que le champ skipper n'est pas fiable\n",
    "    #dico['Skipper']=html.split('/')[-1].replace('-',' ').lower()\n",
    "    #On créé la cle cle qui contient le nom de famille pour la jointure des 2 DF\n",
    "    dico['Cle'] = html.split('/')[-1].replace('-',' ').lower().split(' ')[-1]\n",
    "    for tag in soup.findAll('li', attrs={'class': 'skipper-boat-list-specs-list__item'}) :\n",
    "        categorie = tag.text.split(':')[0]\n",
    "        text = tag.text.split(':')[1][1:]\n",
    "        if categorie == 'Numéro de voile ':\n",
    "            #dico['N']= (int(re.compile(masque2).search(text).group()))\n",
    "            #print(dico['N'])\n",
    "            pass\n",
    "        elif categorie == 'Architecte ':\n",
    "            dico['constructeur'] = text\n",
    "            pass\n",
    "        elif categorie == 'Chantier ':\n",
    "            dico['chantier'] = text\n",
    "            pass\n",
    "        elif categorie == 'Date de lancement ':\n",
    "            dico['age'] = 2020-int(text.split(' ')[-1])\n",
    "            pass\n",
    "        elif categorie == 'Longueur ':\n",
    "            dico['longueur'] = float(re.compile(masque).search(text).group().replace(',','.'))\n",
    "            pass\n",
    "        elif categorie == 'Largeur ':\n",
    "            dico['largeur'] = float(re.compile(masque).search(text).group().replace(',','.'))\n",
    "            pass\n",
    "        elif categorie == \"Tirant d'eau \":\n",
    "            dico['tirant'] = float(re.compile(masque).search(text).group().replace(',','.'))\n",
    "            pass\n",
    "        elif categorie == 'Déplacement (poids) ':\n",
    "            if text.lower() != 'nc':\n",
    "                dico['poids'] = float(re.compile(masque2).search(text).group())\n",
    "            else : \n",
    "                #dico['poids'] = 'na'\n",
    "                pass\n",
    "        elif categorie == 'Nombre de dérives ':\n",
    "            if text[:4] == 'foil':\n",
    "                dico['foils'] = True\n",
    "                #dico['derive'] = False\n",
    "            else :\n",
    "                dico['foils'] = False\n",
    "                #dico['derive'] = float(re.compile(masque2).search(text).group())\n",
    "            pass\n",
    "        elif categorie == 'Hauteur mât ':\n",
    "            dico['mat'] = int(re.compile(masque2).search(text).group())\n",
    "            pass\n",
    "        elif categorie == 'Voile quille ':\n",
    "            dico['voile_quille'] = text\n",
    "            pass\n",
    "        elif categorie == 'Surface de voiles au près ':\n",
    "            dico['surface'] = int(re.compile(masque2).search(text).group())\n",
    "            pass\n",
    "        elif categorie == 'Surface de voiles au portant ':\n",
    "            dico['surface_portant'] = int(re.compile(masque2).search(text).group())\n",
    "            pass\n",
    "    \n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_fiche_tech(html):\n",
    "    \"\"\"\n",
    "    On scrap la page principale pour récupérer l'url de tous les skippers.\n",
    "    On parcours chaque page du skipper / bateau\n",
    "    On scrap chaque page du bateau et on concatène le dico associé.\n",
    "    \"\"\"\n",
    "    urls = get_url_skippers(html)\n",
    "    fiches_tech_boat=[]\n",
    "    check=len(urls)\n",
    "    if __name__ ==  '__main__': \n",
    "        with Pool() as p :\n",
    "            fiches_tech_boat = p.map(mptools.Get_Skipper,[f\"{html}{url}\" for url in urls])        \n",
    "    print(f\"{len(urls)} fiches traitées\")\n",
    "    return fiches_tech_boat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_compl(df):\n",
    "    \"\"\"\n",
    "    On va produire un DF contenant uniquement les colonnes cle/Num_voile/Skipper\n",
    "    avec des lignes distinct (uniquement 33) en s'appuyant sur le DF du classement\n",
    "    On fait un join en s'appuyant sur les deux champ clé passé en index.\n",
    "    on retourne le df_tech compléter du champ Skipper et indexé sur le Num de voile.\n",
    "    NB : c'est pas génial, mais ça marche ...\n",
    "    \"\"\"\n",
    "    df3 = pd.DataFrame.join(df.set_index('Cle')\n",
    "                        ,df_clas.drop_duplicates(subset=['Skipper'])[df_clas.columns[[2,3,22]]].set_index('Cle')\n",
    "                       )\n",
    "    return df3.set_index(df3.columns[-2]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_tech(html):\n",
    "    \"\"\"\n",
    "    On produit un premier DF contenant tous les détails technique.\n",
    "    On corrige quelques NA\n",
    "    On rajouter les 2 colonnes (Num_Voile et Skipper) manquante\n",
    "    On utilise LabelBinarizer pour transformer la colonne voile_quille en binaire pour les modèles de prédiction\n",
    "        Non utilisé mais en prévision\n",
    "    On rend le dis DF qui sera toujours nommé df_tech dans le reste du tp\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(prod_fiche_tech(html))\n",
    "    #On corrige avec la moyenne un champ manquant dans 2 colonnes\n",
    "    df['poids']=df['poids'].fillna('8')\n",
    "    df['voile_quille']=df['voile_quille'].fillna('acier forgé')\n",
    "    #On transforme un champ catégorielle en champ binaire (non utilisé par la suite)\n",
    "    new_data = LabelBinarizer().fit_transform(df.voile_quille)\n",
    "    label = sorted(df.voile_quille.unique())\n",
    "    df[label]=new_data\n",
    "    #On rajoute 2 colonnes qui corrige des données technique non fiable sur le site.\n",
    "    df = prod_compl(df)\n",
    "    #On sauvegarde le DF technique comme objet pour l'utiliser ailleurs\n",
    "    pickle.Pickler(open(\"df_tech.pkl\",'wb')).dump(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 fiches traitées\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tech = get_df_tech(html)\n",
    "df_tech.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
