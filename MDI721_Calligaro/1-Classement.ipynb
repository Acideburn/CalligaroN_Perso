{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calligaro nicolas\n",
    "## Création des données sur la course\n",
    "Readme :\\\n",
    "Ce jupyter se concentre sur récupérer les données du classement et produire un DataFrame exploitable.\\\n",
    "Runner l'ensemble du jupyter. vérifier que la dernière cellule et ok (si erreur lors du dl des fichiers xls relancer le jupyter)\\\n",
    "la dernière cellule est le 'main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir,mkdir\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "import mptools\n",
    "header = [\"Rang\",\"Nat. / Voile\",\"Skipper / Bateau\",\"Heure FR\",\"Latitude\",\"Longitude\",\"1 Cap\",\"1 Vitesse\",\"1 VMG\"\n",
    "      ,\"1 Distance\",\"2 Cap\",\"2 Vitesse\",\"2 VMG\",\"2 Distance\",\"3 Cap\",\"3 Vitesse\",\"3 VMG\",\"3 Distance\",\"DTF\",\"DTL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_xls(html):\n",
    "    \"\"\"\n",
    "    Fonction qui va télécharger en produire les fichiers xls dans le dossier temp\n",
    "    voir le fichier mptools.py qui contient le code du multiprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    check=0\n",
    "    urls=[]\n",
    "    r = requests.get(f'{html}/fr/classement')\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    for tag in soup.findAll('option'):\n",
    "        if tag.attrs['value'] :\n",
    "            urls.append(f\"{html}/download-race-data/vendeeglobe_{tag.attrs['value']}.xlsx\")\n",
    "    check = len(urls)\n",
    "    if __name__ ==  '__main__': \n",
    "        with Pool() as p :\n",
    "            done = p.map(mptools.Get_Xlsx,[url for url in urls])\n",
    "    for i in done :\n",
    "        if not i:\n",
    "            check-=1\n",
    "    print (f\"on a traité {check} fichiers / sur {len(urls)} prévus\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_df (fichier):\n",
    "    \"\"\"\n",
    "        On ouvre le fichier passé en entré\n",
    "        On traite le header et les ligne en trop (en haut et en bas)\n",
    "        On produit notre propre entête en retirant celle du fichier (on pourrait faire mieux)\n",
    "        On récupère la date et l'heure de l'extraction dans le nom du fichier\n",
    "        On produit 3 nouvelles colonnes :\n",
    "            1 contenant la date (sans l'heure)\n",
    "            2 contenant l'heure (sans la date)\n",
    "            3 contenant la date complète (jour, heure)\n",
    "        \n",
    "    \"\"\"\n",
    "    raw_df = pd.read_excel (f\"temp/{fichier}\"\n",
    "                            ,header = None\n",
    "                            ,skiprows = 5\n",
    "                            ,names = header\n",
    "                            ,skipfooter = 4    \n",
    "                            ,na_values = ['RET','NL']\n",
    "                          )\n",
    "    heure_fichier = fichier[:-5]\n",
    "    if heure_fichier == '20201108_140000':#On attrape le 1er xls\n",
    "        #On donne a tout le monde le bon point de départ\n",
    "        raw_df['Longitude']= raw_df['Longitude'].fillna(method='ffill')\n",
    "        raw_df['Latitude']= raw_df['Latitude'].fillna(method='ffill')\n",
    "        #On donne a tout le monde la bonne distance parcourus de départ\n",
    "        raw_df['2 Distance']= \"0.0 nm\"\n",
    "    #On rajoute la date complète lié au nom du fichier\n",
    "    raw_df.insert (3,'Date full',pd.to_datetime(heure_fichier, format = '%Y%m%d_%H%M%S',errors = 'coerce'))\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df (df):\n",
    "    \"\"\"\n",
    "        On travail sur les colonnes du DF (voir chaque commentaire)\n",
    "    \"\"\"\n",
    "    #la colone heure est inutile.\n",
    "    df.drop(['Heure FR'], axis = 1, inplace = True)\n",
    "    #On subdivise la colonne Nat / voile en deux colonnes. puis on retire la dite colonne\n",
    "    #On fera attention le champ peut etre en 2 ou 3 partie\n",
    "    df.insert (2,'Nationalité',df[df.columns[1]].apply(lambda x : str(x.split()[-2:-1][0])))\n",
    "    df.insert (3,'Num_voile',df[df.columns[1]].apply(lambda x : int(x.split()[-1:][0])))\n",
    "    df.drop(['Nat. / Voile'], axis = 1, inplace = True)\n",
    "    #on subdivise la colone Skipper / bateau en deux colonnes. puis on retire la dite colonne.\n",
    "    df.insert (4,'Skipper',df[df.columns[3]].apply(lambda x : str(x.split('\\n')[0]).replace('é','e').lower()))\n",
    "    #On créé la colonne clé qui contient le nom de famille pour la jointure des 2 DF\n",
    "    df['Cle']=df['Skipper'].apply(lambda x: x.split(' ')[-1])\n",
    "    df.insert (5,'Bateau',df[df.columns[3]].apply(lambda x : str(x.split('\\n')[1])))\n",
    "    df.drop(['Skipper / Bateau'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    #On trie le DF par numéro de voile et par les dates.\n",
    "    df = df.sort_values(by = ['Num_voile','Date full'])\n",
    "    #On remplace tous les na trouvé et récupérant la valeur du dessus\n",
    "    #NB : toute les voiles ont eu une valeur de départ\n",
    "    #Pour les bateaux retiré, on considère qu'il reste sur place\n",
    "    df['Longitude']= df['Longitude'].fillna(method='pad')\n",
    "    df['Latitude']= df['Latitude'].fillna(method='ffill')\n",
    "    #On change le modèle des coordonées\n",
    "    df['Latitude'] = df['Latitude'].apply(lambda x : coord(x))\n",
    "    df['Longitude'] = df['Longitude'].apply(lambda x : coord(x))\n",
    "    #On retire les lignes avec un NA\n",
    "    df= df.dropna()\n",
    "    #On passe la colone Rang en numérique\n",
    "    df['Rang'] = pd.to_numeric(df['Rang'])\n",
    "    df = df.astype({\"Rang\": int})#on force le type int au lieu de float\n",
    "    #On retire toutes les unités de mesure et on cast en float\n",
    "    for i in list(df.columns)[8:-1]:\n",
    "        df[i] = df[i].apply(lambda x : get_out_unit(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord(champ):\n",
    "    \"\"\"\n",
    "        On convertie le champ long et lat en coord gps\n",
    "        avec la formule :\n",
    "        W/S = -1\n",
    "        X°Y'Z\"= X+(Y/60)+(Z/3600)\n",
    "    \"\"\"\n",
    "    \n",
    "    if str(champ) == 'nan' :\n",
    "        #print(\"Na détecté\")\n",
    "        return False\n",
    "    else : \n",
    "        #print (f\"{champ}\")\n",
    "        #print (f\"{int(champ[:2])} {int(champ[3:5])/60:.3} {int(champ[6:8])/3600:.3}\")\n",
    "        degree = int(champ[:2])\n",
    "        minute = int(champ[3:5])/60\n",
    "        seconde = int(champ[6:8])/3600\n",
    "        if champ[-1] == 'S' or champ[-1] == 'W' :\n",
    "            card =  -1\n",
    "        else :\n",
    "            card = 1\n",
    "    return float(card * (degree + minute + seconde))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_unit(text):\n",
    "    \"\"\"\n",
    "    On filtre et ne rend que les valeurs numériques\n",
    "    Cela retire toutes les unités de mesure, pour avoir des champs numérique\n",
    "    \"\"\"\n",
    "    masque = \"[0-9]*[.]*[0-9]\"\n",
    "    return float(re.compile(masque).search(text).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_classement(html):\n",
    "    \"\"\"\n",
    "    On produit un DF vierge avec la bonne entête.\n",
    "    On lit chaque fichier, on en produit un DF\n",
    "    On les concatène entre eux\n",
    "    On les nettoye\n",
    "    On retourne de DF du Classement\n",
    "    Le DF sera mis en mémoire et appeler a chaque début de jupyter\n",
    "    \"\"\"\n",
    "    #On créé le dossier temp si possible.\n",
    "    try :\n",
    "        mkdir('temp')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    #On télécharge tous les fichiers excels\n",
    "    prod_xls(html)\n",
    "    #on créer un DF vide mais avec les bonnes colonnes.\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    #On parcours chaque fichier du dossier\n",
    "    for file in listdir(f\"temp/\"):\n",
    "        #On produit un DF brut avec quelques modifications\n",
    "        df1 = prod_df(file)\n",
    "        #On concatène l'ensemble des DF en un seul\n",
    "        df = df1.append(df)\n",
    "    #On nettoye les données\n",
    "    df = prepare_df(df)\n",
    "    #On sauvegarde le DF classement comme objet pour l'utiliser ailleurs\n",
    "    pickle.Pickler(open(\"df_clas.pkl\",'wb')).dump(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on a traité 129 fichiers / sur 129 prévus\n"
     ]
    }
   ],
   "source": [
    "df_clas= get_df_classement('https://www.vendeeglobe.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4142, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clas.shape#,df_clas.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
